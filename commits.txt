{
  "message": "[SPARK-21125][PYTHON] Extend setJobDescription to PySpark and JavaSpark APIs\n\n## What changes were proposed in this pull request?\n\nExtend setJobDescription to PySpark and JavaSpark APIs\n\nSPARK-21125\n\n## How was this patch tested?\n\nTesting was done by running a local Spark shell on the built UI. I originally had added a unit test but the PySpark context cannot easily access the Scala Spark Context's private variable with the Job Description key so I omitted the test, due to the simplicity of this addition.\n\nAlso ran the existing tests.\n\n# Misc\n\nThis contribution is my original work and that I license the work to the project under the project's open source license.\n\nAuthor: sjarvie <sjarvie@uber.com>\n\nCloses #18332 from sjarvie/add_python_set_job_description."
}
{
  "message": "[SPARK-21147][SS] Throws an analysis exception when a user-specified schema is given in socket/rate sources\n\n## What changes were proposed in this pull request?\n\nThis PR proposes to throw an exception if a schema is provided by user to socket source as below:\n\n**socket source**\n\n```scala\nimport org.apache.spark.sql.types._\n\nval userSpecifiedSchema = StructType(\n  StructField(\"name\", StringType) ::\n  StructField(\"area\", StringType) :: Nil)\nval df = spark.readStream.format(\"socket\").option(\"host\", \"localhost\").option(\"port\", 9999).schema(userSpecifiedSchema).load\ndf.printSchema\n```\n\nBefore\n\n```\nroot\n |-- value: string (nullable = true)\n```\n\nAfter\n\n```\norg.apache.spark.sql.AnalysisException: The socket source does not support a user-specified schema.;\n  at org.apache.spark.sql.execution.streaming.TextSocketSourceProvider.sourceSchema(socket.scala:199)\n  at org.apache.spark.sql.execution.datasources.DataSource.sourceSchema(DataSource.scala:192)\n  at org.apache.spark.sql.execution.datasources.DataSource.sourceInfo$lzycompute(DataSource.scala:87)\n  at org.apache.spark.sql.execution.datasources.DataSource.sourceInfo(DataSource.scala:87)\n  at org.apache.spark.sql.execution.streaming.StreamingRelation$.apply(StreamingRelation.scala:30)\n  at org.apache.spark.sql.streaming.DataStreamReader.load(DataStreamReader.scala:150)\n  ... 50 elided\n```\n\n**rate source**\n\n```scala\nspark.readStream.format(\"rate\").schema(spark.range(1).schema).load().printSchema()\n```\n\nBefore\n\n```\nroot\n |-- timestamp: timestamp (nullable = true)\n |-- value: long (nullable = true)`\n```\n\nAfter\n\n```\norg.apache.spark.sql.AnalysisException: The rate source does not support a user-specified schema.;\n  at org.apache.spark.sql.execution.streaming.RateSourceProvider.sourceSchema(RateSourceProvider.scala:57)\n  at org.apache.spark.sql.execution.datasources.DataSource.sourceSchema(DataSource.scala:192)\n  at org.apache.spark.sql.execution.datasources.DataSource.sourceInfo$lzycompute(DataSource.scala:87)\n  at org.apache.spark.sql.execution.datasources.DataSource.sourceInfo(DataSource.scala:87)\n  at org.apache.spark.sql.execution.streaming.StreamingRelation$.apply(StreamingRelation.scala:30)\n  at org.apache.spark.sql.streaming.DataStreamReader.load(DataStreamReader.scala:150)\n  ... 48 elided\n```\n\n## How was this patch tested?\n\nUnit test in `TextSocketStreamSuite` and `RateSourceSuite`.\n\nAuthor: hyukjinkwon <gurwls223@gmail.com>\n\nCloses #18365 from HyukjinKwon/SPARK-21147."
}
{
  "message": "[SPARK-20917][ML][SPARKR] SparkR supports string encoding consistent with R\n\n## What changes were proposed in this pull request?\n\nAdd `stringIndexerOrderType` to `spark.glm` and `spark.survreg` to support string encoding that is consistent with default R.\n\n## How was this patch tested?\nnew tests\n\nAuthor: actuaryzhang <actuaryzhang10@gmail.com>\n\nCloses #18140 from actuaryzhang/sparkRFormula."
}
{
  "message": "[SPARK-17851][SQL][TESTS] Make sure all test sqls in catalyst pass checkAnalysis\n\n## What changes were proposed in this pull request?\n\nCurrently we have several tens of test sqls in catalyst will fail at `SimpleAnalyzer.checkAnalysis`, we should make sure they are valid.\n\nThis PR makes the following changes:\n1. Apply `checkAnalysis` on plans that tests `Optimizer` rules, but don't require the testcases for `Parser`/`Analyzer` pass `checkAnalysis`;\n2. Fix testcases for `Optimizer` that would have fall.\n## How was this patch tested?\n\nApply `SimpleAnalyzer.checkAnalysis` on plans in `PlanTest.comparePlans`, update invalid test cases.\n\nAuthor: Xingbo Jiang <xingbo.jiang@databricks.com>\nAuthor: jiangxingbo <jiangxb1987@gmail.com>\n\nCloses #15417 from jiangxb1987/cptest."
}
{
  "message": "[MINOR][DOC] modified issue link and updated status\n\n## What changes were proposed in this pull request?\n\nThis PR aims to clarify some outdated comments that i found at **spark-catalyst** and **spark-sql** pom files. Maven bug still happening and in order to track it I have updated the issue link and also the status of the issue.\n\nAuthor: Marcos P <mpenate@stratio.com>\n\nCloses #18374 from mpenate/fix/mng-3559-comment."
}
{
  "message": "[MINOR][DOCS] Add lost <tr> tag for configuration.md\n\n## What changes were proposed in this pull request?\n\nAdd lost `<tr>` tag for `configuration.md`.\n\n## How was this patch tested?\nN/A\n\nAuthor: Yuming Wang <wgyumg@gmail.com>\n\nCloses #18372 from wangyum/docs-missing-tr."
}
{
  "message": "[SPARK-20640][CORE] Make rpc timeout and retry for shuffle registration configurable.\n\n## What changes were proposed in this pull request?\n\nCurrently the shuffle service registration timeout and retry has been hardcoded. This works well for small workloads but under heavy workload when the shuffle service is busy transferring large amount of data we see significant delay in responding to the registration request, as a result we often see the executors fail to register with the shuffle service, eventually failing the job. We need to make these two parameters configurable.\n\n## How was this patch tested?\n\n* Updated `BlockManagerSuite` to test registration timeout and max attempts configuration actually works.\n\ncc sitalkedia\n\nAuthor: Li Yichao <lyc@zhihu.com>\n\nCloses #18092 from liyichao/SPARK-20640."
}
{
  "message": "[SPARK-10655][SQL] Adding additional data type mappings to jdbc DB2dialect.\n\nThis patch adds DB2 specific data type mappings for decfloat, real, xml , and timestamp with time zone (DB2Z specific type)  types on read and for byte, short data types  on write to the to jdbc data source DB2 dialect. Default mapping does not work for these types when reading/writing from DB2 database.\n\nAdded docker test, and a JDBC unit test case.\n\nAuthor: sureshthalamati <suresh.thalamati@gmail.com>\n\nCloses #9162 from sureshthalamati/db2dialect_enhancements-spark-10655."
}
{
  "message": "[SPARK-21103][SQL] QueryPlanConstraints should be part of LogicalPlan\n\n## What changes were proposed in this pull request?\nQueryPlanConstraints should be part of LogicalPlan, rather than QueryPlan, since the constraint framework is only used for query plan rewriting and not for physical planning.\n\n## How was this patch tested?\nShould be covered by existing tests, since it is a simple refactoring.\n\nAuthor: Reynold Xin <rxin@databricks.com>\n\nCloses #18310 from rxin/SPARK-21103."
}
{
  "message": "[SPARK-21150][SQL] Persistent view stored in Hive metastore should be case preserving\n\n## What changes were proposed in this pull request?\n\nThis is a regression in Spark 2.2. In Spark 2.2, we introduced a new way to resolve persisted view: https://issues.apache.org/jira/browse/SPARK-18209 , but this makes the persisted view non case-preserving because we store the schema in hive metastore directly. We should follow data source table and store schema in table properties.\n\n## How was this patch tested?\n\nnew regression test\n\nAuthor: Wenchen Fan <wenchen@databricks.com>\n\nCloses #18360 from cloud-fan/view."
}
{
  "message": "[SPARK-20989][CORE] Fail to start multiple workers on one host if external shuffle service is enabled in standalone mode\n\n## What changes were proposed in this pull request?\n\nIn standalone mode, if we enable external shuffle service by setting `spark.shuffle.service.enabled` to true, and then we try to start multiple workers on one host(by setting `SPARK_WORKER_INSTANCES=3` in spark-env.sh, and then run `sbin/start-slaves.sh`), we can only launch one worker on each host successfully and the rest of the workers fail to launch.\nThe reason is the port of external shuffle service if configed by `spark.shuffle.service.port`, so currently we could start no more than one external shuffle service on each host. In our case, each worker tries to start a external shuffle service, and only one of them succeeded doing this.\n\nWe should give explicit reason of failure instead of fail silently.\n\n## How was this patch tested?\nManually test by the following steps:\n1. SET `SPARK_WORKER_INSTANCES=1` in `conf/spark-env.sh`;\n2. SET `spark.shuffle.service.enabled` to `true` in `conf/spark-defaults.conf`;\n3. Run `sbin/start-all.sh`.\n\nBefore the change, you will see no error in the command line, as the following:\n```\nstarting org.apache.spark.deploy.master.Master, logging to /Users/xxx/workspace/spark/logs/spark-xxx-org.apache.spark.deploy.master.Master-1-xxx.local.out\nlocalhost: starting org.apache.spark.deploy.worker.Worker, logging to /Users/xxx/workspace/spark/logs/spark-xxx-org.apache.spark.deploy.worker.Worker-1-xxx.local.out\nlocalhost: starting org.apache.spark.deploy.worker.Worker, logging to /Users/xxx/workspace/spark/logs/spark-xxx-org.apache.spark.deploy.worker.Worker-2-xxx.local.out\nlocalhost: starting org.apache.spark.deploy.worker.Worker, logging to /Users/xxx/workspace/spark/logs/spark-xxx-org.apache.spark.deploy.worker.Worker-3-xxx.local.out\n```\nAnd you can see in the webUI that only one worker is running.\n\nAfter the change, you get explicit error messages in the command line:\n```\nstarting org.apache.spark.deploy.master.Master, logging to /Users/xxx/workspace/spark/logs/spark-xxx-org.apache.spark.deploy.master.Master-1-xxx.local.out\nlocalhost: starting org.apache.spark.deploy.worker.Worker, logging to /Users/xxx/workspace/spark/logs/spark-xxx-org.apache.spark.deploy.worker.Worker-1-xxx.local.out\nlocalhost: failed to launch: nice -n 0 /Users/xxx/workspace/spark/bin/spark-class org.apache.spark.deploy.worker.Worker --webui-port 8081 spark://xxx.local:7077\nlocalhost:   17/06/13 23:24:53 INFO SecurityManager: Changing view acls to: xxx\nlocalhost:   17/06/13 23:24:53 INFO SecurityManager: Changing modify acls to: xxx\nlocalhost:   17/06/13 23:24:53 INFO SecurityManager: Changing view acls groups to:\nlocalhost:   17/06/13 23:24:53 INFO SecurityManager: Changing modify acls groups to:\nlocalhost:   17/06/13 23:24:53 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(xxx); groups with view permissions: Set(); users  with modify permissions: Set(xxx); groups with modify permissions: Set()\nlocalhost:   17/06/13 23:24:54 INFO Utils: Successfully started service 'sparkWorker' on port 63354.\nlocalhost:   Exception in thread \"main\" java.lang.IllegalArgumentException: requirement failed: Start multiple worker on one host failed because we may launch no more than one external shuffle service on each host, please set spark.shuffle.service.enabled to false or set SPARK_WORKER_INSTANCES to 1 to resolve the conflict.\nlocalhost:   \tat scala.Predef$.require(Predef.scala:224)\nlocalhost:   \tat org.apache.spark.deploy.worker.Worker$.main(Worker.scala:752)\nlocalhost:   \tat org.apache.spark.deploy.worker.Worker.main(Worker.scala)\nlocalhost: full log in /Users/xxx/workspace/spark/logs/spark-xxx-org.apache.spark.deploy.worker.Worker-1-xxx.local.out\nlocalhost: starting org.apache.spark.deploy.worker.Worker, logging to /Users/xxx/workspace/spark/logs/spark-xxx-org.apache.spark.deploy.worker.Worker-2-xxx.local.out\nlocalhost: failed to launch: nice -n 0 /Users/xxx/workspace/spark/bin/spark-class org.apache.spark.deploy.worker.Worker --webui-port 8082 spark://xxx.local:7077\nlocalhost:   17/06/13 23:24:56 INFO SecurityManager: Changing view acls to: xxx\nlocalhost:   17/06/13 23:24:56 INFO SecurityManager: Changing modify acls to: xxx\nlocalhost:   17/06/13 23:24:56 INFO SecurityManager: Changing view acls groups to:\nlocalhost:   17/06/13 23:24:56 INFO SecurityManager: Changing modify acls groups to:\nlocalhost:   17/06/13 23:24:56 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(xxx); groups with view permissions: Set(); users  with modify permissions: Set(xxx); groups with modify permissions: Set()\nlocalhost:   17/06/13 23:24:56 INFO Utils: Successfully started service 'sparkWorker' on port 63359.\nlocalhost:   Exception in thread \"main\" java.lang.IllegalArgumentException: requirement failed: Start multiple worker on one host failed because we may launch no more than one external shuffle service on each host, please set spark.shuffle.service.enabled to false or set SPARK_WORKER_INSTANCES to 1 to resolve the conflict.\nlocalhost:   \tat scala.Predef$.require(Predef.scala:224)\nlocalhost:   \tat org.apache.spark.deploy.worker.Worker$.main(Worker.scala:752)\nlocalhost:   \tat org.apache.spark.deploy.worker.Worker.main(Worker.scala)\nlocalhost: full log in /Users/xxx/workspace/spark/logs/spark-xxx-org.apache.spark.deploy.worker.Worker-2-xxx.local.out\nlocalhost: starting org.apache.spark.deploy.worker.Worker, logging to /Users/xxx/workspace/spark/logs/spark-xxx-org.apache.spark.deploy.worker.Worker-3-xxx.local.out\nlocalhost: failed to launch: nice -n 0 /Users/xxx/workspace/spark/bin/spark-class org.apache.spark.deploy.worker.Worker --webui-port 8083 spark://xxx.local:7077\nlocalhost:   17/06/13 23:24:59 INFO SecurityManager: Changing view acls to: xxx\nlocalhost:   17/06/13 23:24:59 INFO SecurityManager: Changing modify acls to: xxx\nlocalhost:   17/06/13 23:24:59 INFO SecurityManager: Changing view acls groups to:\nlocalhost:   17/06/13 23:24:59 INFO SecurityManager: Changing modify acls groups to:\nlocalhost:   17/06/13 23:24:59 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(xxx); groups with view permissions: Set(); users  with modify permissions: Set(xxx); groups with modify permissions: Set()\nlocalhost:   17/06/13 23:24:59 INFO Utils: Successfully started service 'sparkWorker' on port 63360.\nlocalhost:   Exception in thread \"main\" java.lang.IllegalArgumentException: requirement failed: Start multiple worker on one host failed because we may launch no more than one external shuffle service on each host, please set spark.shuffle.service.enabled to false or set SPARK_WORKER_INSTANCES to 1 to resolve the conflict.\nlocalhost:   \tat scala.Predef$.require(Predef.scala:224)\nlocalhost:   \tat org.apache.spark.deploy.worker.Worker$.main(Worker.scala:752)\nlocalhost:   \tat org.apache.spark.deploy.worker.Worker.main(Worker.scala)\nlocalhost: full log in /Users/xxx/workspace/spark/logs/spark-xxx-org.apache.spark.deploy.worker.Worker-3-xxx.local.out\n```\n\nAuthor: Xingbo Jiang <xingbo.jiang@databricks.com>\n\nCloses #18290 from jiangxb1987/start-slave."
}
{
  "message": "[SPARK-20929][ML] LinearSVC should use its own threshold param\n\n## What changes were proposed in this pull request?\n\nLinearSVC should use its own threshold param, rather than the shared one, since it applies to rawPrediction instead of probability.  This PR changes the param in the Scala, Python and R APIs.\n\n## How was this patch tested?\n\nNew unit test to make sure the threshold can be set to any Double value.\n\nAuthor: Joseph K. Bradley <joseph@databricks.com>\n\nCloses #18151 from jkbradley/ml-2.2-linearsvc-cleanup."
}
{
  "message": "[SPARK-20889][SPARKR] Grouped documentation for AGGREGATE column methods\n\n## What changes were proposed in this pull request?\nGrouped documentation for the aggregate functions for Column.\n\nAuthor: actuaryzhang <actuaryzhang10@gmail.com>\n\nCloses #18025 from actuaryzhang/sparkRDoc4."
}
{
  "message": "[SPARK-21133][CORE] Fix HighlyCompressedMapStatus#writeExternal throws NPE\n\n## What changes were proposed in this pull request?\n\nFix HighlyCompressedMapStatus#writeExternal NPE:\n```\n17/06/18 15:00:27 ERROR Utils: Exception encountered\njava.lang.NullPointerException\n        at org.apache.spark.scheduler.HighlyCompressedMapStatus$$anonfun$writeExternal$2.apply$mcV$sp(MapStatus.scala:171)\n        at org.apache.spark.scheduler.HighlyCompressedMapStatus$$anonfun$writeExternal$2.apply(MapStatus.scala:167)\n        at org.apache.spark.scheduler.HighlyCompressedMapStatus$$anonfun$writeExternal$2.apply(MapStatus.scala:167)\n        at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1303)\n        at org.apache.spark.scheduler.HighlyCompressedMapStatus.writeExternal(MapStatus.scala:167)\n        at java.io.ObjectOutputStream.writeExternalData(ObjectOutputStream.java:1459)\n        at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1430)\n        at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)\n        at java.io.ObjectOutputStream.writeArray(ObjectOutputStream.java:1378)\n        at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1174)\n        at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348)\n        at org.apache.spark.MapOutputTracker$$anonfun$serializeMapStatuses$1.apply$mcV$sp(MapOutputTracker.scala:617)\n        at org.apache.spark.MapOutputTracker$$anonfun$serializeMapStatuses$1.apply(MapOutputTracker.scala:616)\n        at org.apache.spark.MapOutputTracker$$anonfun$serializeMapStatuses$1.apply(MapOutputTracker.scala:616)\n        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1337)\n        at org.apache.spark.MapOutputTracker$.serializeMapStatuses(MapOutputTracker.scala:619)\n        at org.apache.spark.MapOutputTrackerMaster.getSerializedMapOutputStatuses(MapOutputTracker.scala:562)\n        at org.apache.spark.MapOutputTrackerMaster$MessageLoop.run(MapOutputTracker.scala:351)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n        at java.lang.Thread.run(Thread.java:745)\n17/06/18 15:00:27 ERROR MapOutputTrackerMaster: java.lang.NullPointerException\njava.io.IOException: java.lang.NullPointerException\n        at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1310)\n        at org.apache.spark.scheduler.HighlyCompressedMapStatus.writeExternal(MapStatus.scala:167)\n        at java.io.ObjectOutputStream.writeExternalData(ObjectOutputStream.java:1459)\n        at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1430)\n        at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)\n        at java.io.ObjectOutputStream.writeArray(ObjectOutputStream.java:1378)\n        at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1174)\n        at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348)\n        at org.apache.spark.MapOutputTracker$$anonfun$serializeMapStatuses$1.apply$mcV$sp(MapOutputTracker.scala:617)\n        at org.apache.spark.MapOutputTracker$$anonfun$serializeMapStatuses$1.apply(MapOutputTracker.scala:616)\n        at org.apache.spark.MapOutputTracker$$anonfun$serializeMapStatuses$1.apply(MapOutputTracker.scala:616)\n        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1337)\n        at org.apache.spark.MapOutputTracker$.serializeMapStatuses(MapOutputTracker.scala:619)\n        at org.apache.spark.MapOutputTrackerMaster.getSerializedMapOutputStatuses(MapOutputTracker.scala:562)\n        at org.apache.spark.MapOutputTrackerMaster$MessageLoop.run(MapOutputTracker.scala:351)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n        at java.lang.Thread.run(Thread.java:745)\nCaused by: java.lang.NullPointerException\n        at org.apache.spark.scheduler.HighlyCompressedMapStatus$$anonfun$writeExternal$2.apply$mcV$sp(MapStatus.scala:171)\n        at org.apache.spark.scheduler.HighlyCompressedMapStatus$$anonfun$writeExternal$2.apply(MapStatus.scala:167)\n        at org.apache.spark.scheduler.HighlyCompressedMapStatus$$anonfun$writeExternal$2.apply(MapStatus.scala:167)\n        at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1303)\n        ... 17 more\n17/06/18 15:00:27 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 10.17.47.20:50188\n17/06/18 15:00:27 ERROR Utils: Exception encountered\njava.lang.NullPointerException\n        at org.apache.spark.scheduler.HighlyCompressedMapStatus$$anonfun$writeExternal$2.apply$mcV$sp(MapStatus.scala:171)\n        at org.apache.spark.scheduler.HighlyCompressedMapStatus$$anonfun$writeExternal$2.apply(MapStatus.scala:167)\n        at org.apache.spark.scheduler.HighlyCompressedMapStatus$$anonfun$writeExternal$2.apply(MapStatus.scala:167)\n        at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1303)\n        at org.apache.spark.scheduler.HighlyCompressedMapStatus.writeExternal(MapStatus.scala:167)\n        at java.io.ObjectOutputStream.writeExternalData(ObjectOutputStream.java:1459)\n        at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1430)\n        at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1178)\n        at java.io.ObjectOutputStream.writeArray(ObjectOutputStream.java:1378)\n        at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1174)\n        at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348)\n        at org.apache.spark.MapOutputTracker$$anonfun$serializeMapStatuses$1.apply$mcV$sp(MapOutputTracker.scala:617)\n        at org.apache.spark.MapOutputTracker$$anonfun$serializeMapStatuses$1.apply(MapOutputTracker.scala:616)\n        at org.apache.spark.MapOutputTracker$$anonfun$serializeMapStatuses$1.apply(MapOutputTracker.scala:616)\n        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1337)\n        at org.apache.spark.MapOutputTracker$.serializeMapStatuses(MapOutputTracker.scala:619)\n        at org.apache.spark.MapOutputTrackerMaster.getSerializedMapOutputStatuses(MapOutputTracker.scala:562)\n        at org.apache.spark.MapOutputTrackerMaster$MessageLoop.run(MapOutputTracker.scala:351)\n        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n        at java.lang.Thread.run(Thread.java:745)\n```\n\n## How was this patch tested?\n\nmanual tests\n\nAuthor: Yuming Wang <wgyumg@gmail.com>\n\nCloses #18343 from wangyum/SPARK-21133."
}
{
  "message": "[INFRA] Close stale PRs.\n\nCloses #18311\nCloses #18278"
}
{
  "message": "[SPARK-21138][YARN] Cannot delete staging dir when the clusters of \"spark.yarn.stagingDir\" and \"spark.hadoop.fs.defaultFS\" are different\n\n## What changes were proposed in this pull request?\n\nWhen I set different clusters for \"spark.hadoop.fs.defaultFS\" and \"spark.yarn.stagingDir\" as follows：\n```\nspark.hadoop.fs.defaultFS  hdfs://tl-nn-tdw.tencent-distribute.com:54310\nspark.yarn.stagingDir hdfs://ss-teg-2-v2/tmp/spark\n```\nThe staging dir can not be deleted, it will prompt following message:\n```\njava.lang.IllegalArgumentException: Wrong FS: hdfs://ss-teg-2-v2/tmp/spark/.sparkStaging/application_1496819138021_77618, expected: hdfs://tl-nn-tdw.tencent-distribute.com:54310\n```\n\n## How was this patch tested?\n\nExisting tests\n\nAuthor: sharkdtu <sharkdtu@tencent.com>\n\nCloses #18352 from sharkdtu/master."
}
{
  "message": "[SPARK-21124][UI] Show correct application user in UI.\n\nThe jobs page currently shows the application user, but it assumes\nthe OS user is the same as the user running the application, which\nmay not be true in all scenarios (e.g., kerberos). While it might be\nuseful to show both in the UI, this change just chooses the application\nuser over the OS user, since the latter can be found in the environment\npage if needed.\n\nTested in live application and in history server.\n\nAuthor: Marcelo Vanzin <vanzin@cloudera.com>\n\nCloses #18331 from vanzin/SPARK-21124."
}
{
  "message": "[MINOR] Fix some typo of the document\n\n## What changes were proposed in this pull request?\n\nFix some typo of the document.\n\n## How was this patch tested?\n\nExisting tests.\n\nPlease review http://spark.apache.org/contributing.html before opening a pull request.\n\nAuthor: Xianyang Liu <xianyang.liu@intel.com>\n\nCloses #18350 from ConeyLiu/fixtypo."
}
{
  "message": "[MINOR][BUILD] Fix Java linter errors\n\n## What changes were proposed in this pull request?\n\nThis PR cleans up a few Java linter errors for Apache Spark 2.2 release.\n\n## How was this patch tested?\n\n```bash\n$ dev/lint-java\nUsing `mvn` from path: /usr/local/bin/mvn\nCheckstyle checks passed.\n```\n\nWe can check the result at Travis CI, [here](https://travis-ci.org/dongjoon-hyun/spark/builds/244297894).\n\nAuthor: Dongjoon Hyun <dongjoon@apache.org>\n\nCloses #18345 from dongjoon-hyun/fix_lint_java_2."
}
{
  "message": "[SPARK-19975][PYTHON][SQL] Add map_keys and map_values functions to Python\n\n## What changes were proposed in this pull request?\n\nThis fix tries to address the issue in SPARK-19975 where we\nhave `map_keys` and `map_values` functions in SQL yet there\nis no Python equivalent functions.\n\nThis fix adds `map_keys` and `map_values` functions to Python.\n\n## How was this patch tested?\n\nThis fix is tested manually (See Python docs for examples).\n\nAuthor: Yong Tang <yong.tang.github@outlook.com>\n\nCloses #17328 from yongtang/SPARK-19975."
}
{
  "message": "[SPARK-21123][DOCS][STRUCTURED STREAMING] Options for file stream source are in a wrong table\n\n## What changes were proposed in this pull request?\n\nThe description for several options of File Source for structured streaming appeared in the File Sink description instead.\n\nThis pull request has two commits: The first includes changes to the version as it appeared in spark 2.1 and the second handled an additional option added for spark 2.2\n\n## How was this patch tested?\n\nBuilt the documentation by SKIP_API=1 jekyll build and visually inspected the structured streaming programming guide.\n\nThe original documentation was written by tdas and lw-lin\n\nAuthor: assafmendelson <assaf.mendelson@gmail.com>\n\nCloses #18342 from assafmendelson/spark-21123."
}
{
  "message": "[SPARK-19688][STREAMING] Not to read `spark.yarn.credentials.file` from checkpoint.\n\n## What changes were proposed in this pull request?\n\nReload the `spark.yarn.credentials.file` property when restarting a streaming application from checkpoint.\n\n## How was this patch tested?\n\nManual tested with 1.6.3 and 2.1.1.\nI didn't test this with master because of some compile problems, but I think it will be the same result.\n\n## Notice\n\nThis should be merged into maintenance branches too.\n\njira: [SPARK-21008](https://issues.apache.org/jira/browse/SPARK-21008)\n\nAuthor: saturday_s <shi.indetail@gmail.com>\n\nCloses #18230 from saturday-shi/SPARK-21008."
}
{
  "message": "[MINOR] Bump SparkR and PySpark version to 2.3.0.\n\n## What changes were proposed in this pull request?\n\n#17753 bumps master branch version to 2.3.0-SNAPSHOT, but it seems SparkR and PySpark version were omitted.\n\nditto of https://github.com/apache/spark/pull/16488 / https://github.com/apache/spark/pull/17523\n\n## How was this patch tested?\n\nN/A\n\nAuthor: hyukjinkwon <gurwls223@gmail.com>\n\nCloses #18341 from HyukjinKwon/r-version."
}
{
  "message": "[SPARK-21132][SQL] DISTINCT modifier of function arguments should not be silently ignored\n\n### What changes were proposed in this pull request?\nWe should not silently ignore `DISTINCT` when they are not supported in the function arguments. This PR is to block these cases and issue the error messages.\n\n### How was this patch tested?\nAdded test cases for both regular functions and window functions\n\nAuthor: Xiao Li <gatorsmile@gmail.com>\n\nCloses #18340 from gatorsmile/firstCount."
}
{
  "message": "[SPARK-19824][CORE] Update JsonProtocol to keep consistent with the UI\n\n## What changes were proposed in this pull request?\n\nFix any inconsistent part in JsonProtocol with the UI.\nThis PR also contains the modifications in #17181\n\n## How was this patch tested?\n\nUpdated JsonProtocolSuite.\n\nBefore this change, localhost:8080/json shows:\n```\n{\n  \"url\" : \"spark://xingbos-MBP.local:7077\",\n  \"workers\" : [ {\n    \"id\" : \"worker-20170615172946-192.168.0.101-49450\",\n    \"host\" : \"192.168.0.101\",\n    \"port\" : 49450,\n    \"webuiaddress\" : \"http://192.168.0.101:8081\",\n    \"cores\" : 8,\n    \"coresused\" : 8,\n    \"coresfree\" : 0,\n    \"memory\" : 15360,\n    \"memoryused\" : 1024,\n    \"memoryfree\" : 14336,\n    \"state\" : \"ALIVE\",\n    \"lastheartbeat\" : 1497519481722\n  }, {\n    \"id\" : \"worker-20170615172948-192.168.0.101-49452\",\n    \"host\" : \"192.168.0.101\",\n    \"port\" : 49452,\n    \"webuiaddress\" : \"http://192.168.0.101:8082\",\n    \"cores\" : 8,\n    \"coresused\" : 8,\n    \"coresfree\" : 0,\n    \"memory\" : 15360,\n    \"memoryused\" : 1024,\n    \"memoryfree\" : 14336,\n    \"state\" : \"ALIVE\",\n    \"lastheartbeat\" : 1497519484160\n  }, {\n    \"id\" : \"worker-20170615172951-192.168.0.101-49469\",\n    \"host\" : \"192.168.0.101\",\n    \"port\" : 49469,\n    \"webuiaddress\" : \"http://192.168.0.101:8083\",\n    \"cores\" : 8,\n    \"coresused\" : 8,\n    \"coresfree\" : 0,\n    \"memory\" : 15360,\n    \"memoryused\" : 1024,\n    \"memoryfree\" : 14336,\n    \"state\" : \"ALIVE\",\n    \"lastheartbeat\" : 1497519486905\n  } ],\n  \"cores\" : 24,\n  \"coresused\" : 24,\n  \"memory\" : 46080,\n  \"memoryused\" : 3072,\n  \"activeapps\" : [ {\n    \"starttime\" : 1497519426990,\n    \"id\" : \"app-20170615173706-0001\",\n    \"name\" : \"Spark shell\",\n    \"user\" : \"xingbojiang\",\n    \"memoryperslave\" : 1024,\n    \"submitdate\" : \"Thu Jun 15 17:37:06 CST 2017\",\n    \"state\" : \"RUNNING\",\n    \"duration\" : 65362\n  } ],\n  \"completedapps\" : [ {\n    \"starttime\" : 1497519250893,\n    \"id\" : \"app-20170615173410-0000\",\n    \"name\" : \"Spark shell\",\n    \"user\" : \"xingbojiang\",\n    \"memoryperslave\" : 1024,\n    \"submitdate\" : \"Thu Jun 15 17:34:10 CST 2017\",\n    \"state\" : \"FINISHED\",\n    \"duration\" : 116895\n  } ],\n  \"activedrivers\" : [ ],\n  \"status\" : \"ALIVE\"\n}\n```\n\nAfter the change:\n```\n{\n  \"url\" : \"spark://xingbos-MBP.local:7077\",\n  \"workers\" : [ {\n    \"id\" : \"worker-20170615175032-192.168.0.101-49951\",\n    \"host\" : \"192.168.0.101\",\n    \"port\" : 49951,\n    \"webuiaddress\" : \"http://192.168.0.101:8081\",\n    \"cores\" : 8,\n    \"coresused\" : 8,\n    \"coresfree\" : 0,\n    \"memory\" : 15360,\n    \"memoryused\" : 1024,\n    \"memoryfree\" : 14336,\n    \"state\" : \"ALIVE\",\n    \"lastheartbeat\" : 1497520292900\n  }, {\n    \"id\" : \"worker-20170615175034-192.168.0.101-49953\",\n    \"host\" : \"192.168.0.101\",\n    \"port\" : 49953,\n    \"webuiaddress\" : \"http://192.168.0.101:8082\",\n    \"cores\" : 8,\n    \"coresused\" : 8,\n    \"coresfree\" : 0,\n    \"memory\" : 15360,\n    \"memoryused\" : 1024,\n    \"memoryfree\" : 14336,\n    \"state\" : \"ALIVE\",\n    \"lastheartbeat\" : 1497520280301\n  }, {\n    \"id\" : \"worker-20170615175037-192.168.0.101-49955\",\n    \"host\" : \"192.168.0.101\",\n    \"port\" : 49955,\n    \"webuiaddress\" : \"http://192.168.0.101:8083\",\n    \"cores\" : 8,\n    \"coresused\" : 8,\n    \"coresfree\" : 0,\n    \"memory\" : 15360,\n    \"memoryused\" : 1024,\n    \"memoryfree\" : 14336,\n    \"state\" : \"ALIVE\",\n    \"lastheartbeat\" : 1497520282884\n  } ],\n  \"aliveworkers\" : 3,\n  \"cores\" : 24,\n  \"coresused\" : 24,\n  \"memory\" : 46080,\n  \"memoryused\" : 3072,\n  \"activeapps\" : [ {\n    \"id\" : \"app-20170615175122-0001\",\n    \"starttime\" : 1497520282115,\n    \"name\" : \"Spark shell\",\n    \"cores\" : 24,\n    \"user\" : \"xingbojiang\",\n    \"memoryperslave\" : 1024,\n    \"submitdate\" : \"Thu Jun 15 17:51:22 CST 2017\",\n    \"state\" : \"RUNNING\",\n    \"duration\" : 10805\n  } ],\n  \"completedapps\" : [ {\n    \"id\" : \"app-20170615175058-0000\",\n    \"starttime\" : 1497520258766,\n    \"name\" : \"Spark shell\",\n    \"cores\" : 24,\n    \"user\" : \"xingbojiang\",\n    \"memoryperslave\" : 1024,\n    \"submitdate\" : \"Thu Jun 15 17:50:58 CST 2017\",\n    \"state\" : \"FINISHED\",\n    \"duration\" : 9876\n  } ],\n  \"activedrivers\" : [ ],\n  \"completeddrivers\" : [ ],\n  \"status\" : \"ALIVE\"\n}\n```\n\nAuthor: Xingbo Jiang <xingbo.jiang@databricks.com>\n\nCloses #18303 from jiangxb1987/json-protocol."
}
{
  "message": "[SPARK-21090][CORE] Optimize the unified memory manager code\n\n## What changes were proposed in this pull request?\n1.In `acquireStorageMemory`, when the Memory Mode is OFF_HEAP ,the `maxOffHeapMemory` should be modified to `maxOffHeapStorageMemory`. after this PR,it will same as ON_HEAP Memory Mode.\nBecause when acquire memory is between `maxOffHeapStorageMemory` and `maxOffHeapMemory`,it will fail surely, so if acquire memory is greater than  `maxOffHeapStorageMemory`(not greater than `maxOffHeapMemory`),we should fail fast.\n2. Borrow memory from execution, `numBytes` modified to `numBytes - storagePool.memoryFree` will be more reasonable.\nBecause we just acquire `(numBytes - storagePool.memoryFree)`, unnecessary borrowed `numBytes` from execution\n\n## How was this patch tested?\nadded unit test case\n\nAuthor: liuxian <liu.xian3@zte.com.cn>\n\nCloses #18296 from 10110346/wip-lx-0614."
}
{
  "message": "[SPARK-20948][SQL] Built-in SQL Function UnaryMinus/UnaryPositive support string type\n\n## What changes were proposed in this pull request?\n\nBuilt-in SQL Function UnaryMinus/UnaryPositive support string type, if it's string type, convert it to double type, after this PR:\n```sql\nspark-sql> select positive('-1.11'), negative('-1.11');\n-1.11   1.11\nspark-sql>\n```\n\n## How was this patch tested?\n\nunit tests\n\nAuthor: Yuming Wang <wgyumg@gmail.com>\n\nCloses #18173 from wangyum/SPARK-20948."
}
{
  "message": "[SPARK-20749][SQL][FOLLOWUP] Support character_length\n\n## What changes were proposed in this pull request?\n\nThe function `char_length` is shorthand for `character_length` function. Both Hive and Postgresql support `character_length`,  This PR add support for `character_length`.\n\nRef:\nhttps://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF#LanguageManualUDF-StringFunctions\nhttps://www.postgresql.org/docs/current/static/functions-string.html\n\n## How was this patch tested?\n\nunit tests\n\nAuthor: Yuming Wang <wgyumg@gmail.com>\n\nCloses #18330 from wangyum/SPARK-20749-character_length."
}
{
  "message": "[SPARK-20892][SPARKR] Add SQL trunc function to SparkR\n\n## What changes were proposed in this pull request?\n\nAdd SQL trunc function\n\n## How was this patch tested?\nstandard test\n\nAuthor: actuaryzhang <actuaryzhang10@gmail.com>\n\nCloses #18291 from actuaryzhang/sparkRTrunc2."
}
{
  "message": "[SPARK-21128][R] Remove both \"spark-warehouse\" and \"metastore_db\" before listing files in R tests\n\n## What changes were proposed in this pull request?\n\nThis PR proposes to list the files in test _after_ removing both \"spark-warehouse\" and \"metastore_db\" so that the next run of R tests pass fine. This is sometimes a bit annoying.\n\n## How was this patch tested?\n\nManually running multiple times R tests via `./R/run-tests.sh`.\n\n**Before**\n\nSecond run:\n\n```\nSparkSQL functions: Spark package found in SPARK_HOME: .../spark\n...............................................................................................................................................................\n...............................................................................................................................................................\n...............................................................................................................................................................\n...............................................................................................................................................................\n...............................................................................................................................................................\n....................................................................................................1234.......................\n\nFailed -------------------------------------------------------------------------\n1. Failure: No extra files are created in SPARK_HOME by starting session and making calls (test_sparkSQL.R#3384)\nlength(list1) not equal to length(list2).\n1/1 mismatches\n[1] 25 - 23 == 2\n\n2. Failure: No extra files are created in SPARK_HOME by starting session and making calls (test_sparkSQL.R#3384)\nsort(list1, na.last = TRUE) not equal to sort(list2, na.last = TRUE).\n10/25 mismatches\nx[16]: \"metastore_db\"\ny[16]: \"pkg\"\n\nx[17]: \"pkg\"\ny[17]: \"R\"\n\nx[18]: \"R\"\ny[18]: \"README.md\"\n\nx[19]: \"README.md\"\ny[19]: \"run-tests.sh\"\n\nx[20]: \"run-tests.sh\"\ny[20]: \"SparkR_2.2.0.tar.gz\"\n\nx[21]: \"metastore_db\"\ny[21]: \"pkg\"\n\nx[22]: \"pkg\"\ny[22]: \"R\"\n\nx[23]: \"R\"\ny[23]: \"README.md\"\n\nx[24]: \"README.md\"\ny[24]: \"run-tests.sh\"\n\nx[25]: \"run-tests.sh\"\ny[25]: \"SparkR_2.2.0.tar.gz\"\n\n3. Failure: No extra files are created in SPARK_HOME by starting session and making calls (test_sparkSQL.R#3388)\nlength(list1) not equal to length(list2).\n1/1 mismatches\n[1] 25 - 23 == 2\n\n4. Failure: No extra files are created in SPARK_HOME by starting session and making calls (test_sparkSQL.R#3388)\nsort(list1, na.last = TRUE) not equal to sort(list2, na.last = TRUE).\n10/25 mismatches\nx[16]: \"metastore_db\"\ny[16]: \"pkg\"\n\nx[17]: \"pkg\"\ny[17]: \"R\"\n\nx[18]: \"R\"\ny[18]: \"README.md\"\n\nx[19]: \"README.md\"\ny[19]: \"run-tests.sh\"\n\nx[20]: \"run-tests.sh\"\ny[20]: \"SparkR_2.2.0.tar.gz\"\n\nx[21]: \"metastore_db\"\ny[21]: \"pkg\"\n\nx[22]: \"pkg\"\ny[22]: \"R\"\n\nx[23]: \"R\"\ny[23]: \"README.md\"\n\nx[24]: \"README.md\"\ny[24]: \"run-tests.sh\"\n\nx[25]: \"run-tests.sh\"\ny[25]: \"SparkR_2.2.0.tar.gz\"\n\nDONE ===========================================================================\n```\n\n**After**\n\nSecond run:\n\n```\nSparkSQL functions: Spark package found in SPARK_HOME: .../spark\n...............................................................................................................................................................\n...............................................................................................................................................................\n...............................................................................................................................................................\n...............................................................................................................................................................\n...............................................................................................................................................................\n...............................................................................................................................\n```\n\nAuthor: hyukjinkwon <gurwls223@gmail.com>\n\nCloses #18335 from HyukjinKwon/SPARK-21128."
}
{
  "message": "[MINOR][R] Add knitr and rmarkdown packages/improve output for version info in AppVeyor tests\n\n## What changes were proposed in this pull request?\n\nThis PR proposes three things as below:\n\n**Install packages per documentation** - this does not affect the tests itself (but CRAN which we are not doing via AppVeyor) up to my knowledge.\n\nThis adds `knitr` and `rmarkdown` per https://github.com/apache/spark/blob/45824fb608930eb461e7df53bb678c9534c183a9/R/WINDOWS.md#unit-tests (please see https://github.com/apache/spark/commit/45824fb608930eb461e7df53bb678c9534c183a9)\n\n**Improve logs/shorten logs** - actually, long logs can be a problem on AppVeyor (e.g., see https://github.com/apache/spark/pull/17873)\n\n`R -e ...` repeats printing R information for each invocation as below:\n\n```\nR version 3.3.1 (2016-06-21) -- \"Bug in Your Hair\"\nCopyright (C) 2016 The R Foundation for Statistical Computing\nPlatform: i386-w64-mingw32/i386 (32-bit)\n\nR is free software and comes with ABSOLUTELY NO WARRANTY.\nYou are welcome to redistribute it under certain conditions.\nType 'license()' or 'licence()' for distribution details.\n\n  Natural language support but running in an English locale\n\nR is a collaborative project with many contributors.\nType 'contributors()' for more information and\n'citation()' on how to cite R or R packages in publications.\n\nType 'demo()' for some demos, 'help()' for on-line help, or\n'help.start()' for an HTML browser interface to help.\nType 'q()' to quit R.\n```\n\nIt looks reducing the call might be slightly better and print out the versions together looks more readable.\n\nBefore:\n\n```\n# R information ...\n> packageVersion('testthat')\n[1] '1.0.2'\n>\n>\n\n# R information ...\n> packageVersion('e1071')\n[1] '1.6.8'\n>\n>\n... 3 more times\n```\n\nAfter:\n\n```\n# R information ...\n> packageVersion('knitr'); packageVersion('rmarkdown'); packageVersion('testthat'); packageVersion('e1071'); packageVersion('survival')\n[1] ‘1.16’\n[1] ‘1.6’\n[1] ‘1.0.2’\n[1] ‘1.6.8’\n[1] ‘2.41.3’\n```\n\n**Add`appveyor.yml`/`dev/appveyor-install-dependencies.ps1` for triggering the test**\n\nChanging this file might break the test, e.g., https://github.com/apache/spark/pull/16927\n\n## How was this patch tested?\n\nBefore (please see https://ci.appveyor.com/project/HyukjinKwon/spark/build/169-master)\nAfter (please see the AppVeyor build in this PR):\n\nAuthor: hyukjinkwon <gurwls223@gmail.com>\n\nCloses #18336 from HyukjinKwon/minor-add-knitr-and-rmarkdown."
}
{
  "message": "[SPARK-21126] The configuration which named \"spark.core.connection.auth.wait.timeout\" hasn't been used in spark\n\n[https://issues.apache.org/jira/browse/SPARK-21126](https://issues.apache.org/jira/browse/SPARK-21126)\nThe configuration which named \"spark.core.connection.auth.wait.timeout\" hasn't been used in spark,so I think it should be removed from configuration.md.\n\nAuthor: liuzhaokun <liu.zhaokun@zte.com.cn>\n\nCloses #18333 from liu-zhaokun/new3."
}
{
  "message": "[SPARK-20338][CORE] Spaces in spark.eventLog.dir are not correctly handled\n\n## What changes were proposed in this pull request?\n\n“spark.eventLog.dir” supports with space characters.\n\n1. Update EventLoggingListenerSuite like `testDir = Utils.createTempDir(namePrefix = s\"history log\")`\n2. Fix EventLoggingListenerSuite tests\n\n## How was this patch tested?\n\nupdate unit tests\n\nAuthor: zuotingbing <zuo.tingbing9@zte.com.cn>\n\nCloses #18285 from zuotingbing/spark-resolveURI."
}
{
  "message": "[SPARK-20931][SQL] ABS function support string type.\n\n## What changes were proposed in this pull request?\n\nABS function support string type. Hive/MySQL support this feature.\n\nRef: https://github.com/apache/hive/blob/4ba713ccd85c3706d195aeef9476e6e6363f1c21/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFAbs.java#L93\n\n## How was this patch tested?\n unit tests\n\nAuthor: Yuming Wang <wgyumg@gmail.com>\n\nCloses #18153 from wangyum/SPARK-20931."
}
{
  "message": "[SPARK-21119][SQL] unset table properties should keep the table comment\n\n## What changes were proposed in this pull request?\n\nPrevious code mistakenly use `table.properties.get(\"comment\")` to read the existing table comment, we should use `table.comment`\n\n## How was this patch tested?\n\nnew regression test\n\nAuthor: Wenchen Fan <wenchen@databricks.com>\n\nCloses #18325 from cloud-fan/unset."
}
{
  "message": "[SPARK-20994] Remove redundant characters in OpenBlocks to save memory for shuffle service.\n\n## What changes were proposed in this pull request?\n\nIn current code, blockIds in `OpenBlocks` are stored in the iterator on shuffle service.\nThere are some redundant characters in  blockId(`\"shuffle_\" + shuffleId + \"_\" + mapId + \"_\" + reduceId`). This pr proposes to improve the footprint and alleviate the memory pressure on shuffle service.\n\nAuthor: jinxing <jinxing6042@126.com>\n\nCloses #18231 from jinxing64/SPARK-20994-v2."
}
{
  "message": "[MINOR][DOCS] Improve Running R Tests docs\n\n## What changes were proposed in this pull request?\n\nUpdate Running R Tests dependence packages to:\n```bash\nR -e \"install.packages(c('knitr', 'rmarkdown', 'testthat', 'e1071', 'survival'), repos='http://cran.us.r-project.org')\"\n```\n\n## How was this patch tested?\nmanual tests\n\nAuthor: Yuming Wang <wgyumg@gmail.com>\n\nCloses #18271 from wangyum/building-spark."
}
{
  "message": "[SPARK-12552][FOLLOWUP] Fix flaky test for \"o.a.s.deploy.master.MasterSuite.master correctly recover the application\"\n\n## What changes were proposed in this pull request?\n\nDue to the RPC asynchronous event processing, The test \"correctly recover the application\" could potentially be failed. The issue could be found in here: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/78126/testReport/org.apache.spark.deploy.master/MasterSuite/master_correctly_recover_the_application/.\n\nSo here fixing this flaky test.\n\n## How was this patch tested?\n\nExisting UT.\n\nCC cloud-fan jiangxb1987 , please help to review, thanks!\n\nAuthor: jerryshao <sshao@hortonworks.com>\n\nCloses #18321 from jerryshao/SPARK-12552-followup."
}
{
  "message": "[SPARK-20749][SQL] Built-in SQL Function Support - all variants of LEN[GTH]\n\n## What changes were proposed in this pull request?\n\nThis PR adds built-in SQL function `BIT_LENGTH()`, `CHAR_LENGTH()`, and `OCTET_LENGTH()` functions.\n\n`BIT_LENGTH()` returns the bit length of the given string or binary expression.\n`CHAR_LENGTH()` returns the length of the given string or binary expression. (i.e. equal to `LENGTH()`)\n`OCTET_LENGTH()` returns the byte length of the given string or binary expression.\n\n## How was this patch tested?\n\nAdded new test suites for these three functions\n\nAuthor: Kazuaki Ishizaki <ishizaki@jp.ibm.com>\n\nCloses #18046 from kiszk/SPARK-20749."
}
{
  "message": "[SPARK-21072][SQL] TreeNode.mapChildren should only apply to the children node.\n\n## What changes were proposed in this pull request?\n\nJust as the function name and comments of `TreeNode.mapChildren` mentioned, the function should be apply to all currently node children. So, the follow code should judge whether it is the children node.\n\nhttps://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/trees/TreeNode.scala#L342\n\n## How was this patch tested?\n\nExisting tests.\n\nAuthor: Xianyang Liu <xianyang.liu@intel.com>\n\nCloses #18284 from ConeyLiu/treenode."
}
{
  "message": "[SPARK-21112][SQL] ALTER TABLE SET TBLPROPERTIES should not overwrite COMMENT\n\n### What changes were proposed in this pull request?\n`ALTER TABLE SET TBLPROPERTIES` should not overwrite `COMMENT` even if the input property does not have the property of `COMMENT`. This PR is to fix the issue.\n\n### How was this patch tested?\nCovered by the existing tests.\n\nAuthor: Xiao Li <gatorsmile@gmail.com>\n\nCloses #18318 from gatorsmile/fixTableComment."
}
{
  "message": "[SPARK-20434][YARN][CORE] Move Hadoop delegation token code from yarn to core\n\n## What changes were proposed in this pull request?\n\nMove Hadoop delegation token code from `spark-yarn` to `spark-core`, so that other schedulers (such as Mesos), may use it.  In order to avoid exposing Hadoop interfaces in spark-core, the new Hadoop delegation token classes are kept private.  In order to provider backward compatiblity, and to allow YARN users to continue to load their own delegation token providers via Java service loading, the old YARN interfaces, as well as the client code that uses them, have been retained.\n\nSummary:\n- Move registered `yarn.security.ServiceCredentialProvider` classes from `spark-yarn` to `spark-core`.  Moved them into a new, private hierarchy under `HadoopDelegationTokenProvider`.  Client code in `HadoopDelegationTokenManager` now loads credentials from a whitelist of three providers (`HadoopFSDelegationTokenProvider`, `HiveDelegationTokenProvider`, `HBaseDelegationTokenProvider`), instead of service loading, which means that users are not able to implement their own delegation token providers, as they are in the `spark-yarn` module.\n\n- The `yarn.security.ServiceCredentialProvider` interface has been kept for backwards compatibility, and to continue to allow YARN users to implement their own delegation token provider implementations.  Client code in YARN now fetches tokens via the new `YARNHadoopDelegationTokenManager` class, which fetches tokens from the core providers through `HadoopDelegationTokenManager`, as well as service loads them from `yarn.security.ServiceCredentialProvider`.\n\nOld Hierarchy:\n\n```\nyarn.security.ServiceCredentialProvider (service loaded)\n  HadoopFSCredentialProvider\n  HiveCredentialProvider\n  HBaseCredentialProvider\nyarn.security.ConfigurableCredentialManager\n```\n\nNew Hierarchy:\n\n```\nHadoopDelegationTokenManager\nHadoopDelegationTokenProvider (not service loaded)\n  HadoopFSDelegationTokenProvider\n  HiveDelegationTokenProvider\n  HBaseDelegationTokenProvider\n\nyarn.security.ServiceCredentialProvider (service loaded)\nyarn.security.YARNHadoopDelegationTokenManager\n```\n## How was this patch tested?\n\nunit tests\n\nAuthor: Michael Gummelt <mgummelt@mesosphere.io>\nAuthor: Dr. Stefan Schimanski <sttts@mesosphere.io>\n\nCloses #17723 from mgummelt/SPARK-20434-refactor-kerberos."
}
{
  "message": "[SPARK-16251][SPARK-20200][CORE][TEST] Flaky test: org.apache.spark.rdd.LocalCheckpointSuite.missing checkpoint block fails with informative message\n\n## What changes were proposed in this pull request?\n\nCurrently we don't wait to confirm the removal of the block from the slave's BlockManager, if the removal takes too much time, we will fail the assertion in this test case.\nThe failure can be easily reproduced if we sleep for a while before we remove the block in BlockManagerSlaveEndpoint.receiveAndReply().\n\n## How was this patch tested?\nN/A\n\nAuthor: Xingbo Jiang <xingbo.jiang@databricks.com>\n\nCloses #18314 from jiangxb1987/LocalCheckpointSuite."
}
{
  "message": "[SPARK-20980][DOCS] update doc to reflect multiLine change\n\n## What changes were proposed in this pull request?\n\ndoc only change\n\n## How was this patch tested?\n\nmanually\n\nAuthor: Felix Cheung <felixcheung_m@hotmail.com>\n\nCloses #18312 from felixcheung/sqljsonwholefiledoc."
}
{
  "message": "[SPARK-18016][SQL][CATALYST] Code Generation: Constant Pool Limit - Class Splitting\n\n## What changes were proposed in this pull request?\n\nThis pull-request exclusively includes the class splitting feature described in #16648. When code for a given class would grow beyond 1600k bytes, a private, nested sub-class is generated into which subsequent functions are inlined. Additional sub-classes are generated as the code threshold is met subsequent times. This code includes 3 changes:\n\n1. Includes helper maps, lists, and functions for keeping track of sub-classes during code generation (included in the `CodeGenerator` class). These helper functions allow nested classes and split functions to be initialized/declared/inlined to the appropriate locations in the various projection classes.\n2. Changes `addNewFunction` to return a string to support instances where a split function is inlined to a nested class and not the outer class (and so must be invoked using the class-qualified name). Uses of `addNewFunction` throughout the codebase are modified so that the returned name is properly used.\n3. Removes instances of the `this` keyword when used on data inside generated classes. All state declared in the outer class is by default global and accessible to the nested classes. However, if a reference to global state in a nested class is prepended with the `this` keyword, it would attempt to reference state belonging to the nested class (which would not exist), rather than the correct variable belonging to the outer class.\n\n## How was this patch tested?\n\nAdded a test case to the `GeneratedProjectionSuite` that increases the number of columns tested in various projections to a threshold that would previously have triggered a `JaninoRuntimeException` for the Constant Pool.\n\nNote: This PR does not address the second Constant Pool issue with code generation (also mentioned in #16648): excess global mutable state. A second PR may be opened to resolve that issue.\n\nAuthor: ALeksander Eskilson <alek.eskilson@cerner.com>\n\nCloses #18075 from bdrillard/class_splitting_only."
}
{
  "message": "[SPARK-20980][SQL] Rename `wholeFile` to `multiLine` for both CSV and JSON\n\n### What changes were proposed in this pull request?\nThe current option name `wholeFile` is misleading for CSV users. Currently, it is not representing a record per file. Actually, one file could have multiple records. Thus, we should rename it. Now, the proposal is `multiLine`.\n\n### How was this patch tested?\nN/A\n\nAuthor: Xiao Li <gatorsmile@gmail.com>\n\nCloses #18202 from gatorsmile/renameCVSOption."
}
{
  "message": "[SPARK-21092][SQL] Wire SQLConf in logical plan and expressions\n\n## What changes were proposed in this pull request?\nIt is really painful to not have configs in logical plan and expressions. We had to add all sorts of hacks (e.g. pass SQLConf explicitly in functions). This patch exposes SQLConf in logical plan, using a thread local variable and a getter closure that's set once there is an active SparkSession.\n\nThe implementation is a bit of a hack, since we didn't anticipate this need in the beginning (config was only exposed in physical plan). The implementation is described in `SQLConf.get`.\n\nIn terms of future work, we should follow up to clean up CBO (remove the need for passing in config).\n\n## How was this patch tested?\nUpdated relevant tests for constraint propagation.\n\nAuthor: Reynold Xin <rxin@databricks.com>\n\nCloses #18299 from rxin/SPARK-21092."
}
{
  "message": "[SPARK-19900][CORE] Remove driver when relaunching.\n\nThis is https://github.com/apache/spark/pull/17888 .\n\nBelow are some spark ui snapshots.\n\nMaster, after worker disconnects:\n\n<img width=\"1433\" alt=\"master_disconnect\" src=\"https://cloud.githubusercontent.com/assets/2576762/26398687/d0ee228e-40ac-11e7-986d-d3b57b87029f.png\">\n\nMaster, after worker reconnects, notice the `running drivers` part:\n\n<img width=\"1412\" alt=\"master_reconnects\" src=\"https://cloud.githubusercontent.com/assets/2576762/26398697/d50735a4-40ac-11e7-80d8-6e9e1cf0b62f.png\">\n\nThis patch, after worker disconnects:\n<img width=\"1412\" alt=\"patch_disconnect\" src=\"https://cloud.githubusercontent.com/assets/2576762/26398009/c015d3dc-40aa-11e7-8bb4-df11a1f66645.png\">\n\nThis patch, after worker reconnects:\n![image](https://cloud.githubusercontent.com/assets/2576762/26398037/d313769c-40aa-11e7-8613-5f157d193150.png)\n\ncc cloud-fan jiangxb1987\n\nAuthor: Li Yichao <lyc@zhihu.com>\n\nCloses #18084 from liyichao/SPARK-19900-1."
}
{
  "message": "[SPARK-21091][SQL] Move constraint code into QueryPlanConstraints\n\n## What changes were proposed in this pull request?\nThis patch moves constraint related code into a separate trait QueryPlanConstraints, so we don't litter QueryPlan with a lot of constraint private functions.\n\n## How was this patch tested?\nThis is a simple move refactoring and should be covered by existing tests.\n\nAuthor: Reynold Xin <rxin@databricks.com>\n\nCloses #18298 from rxin/SPARK-21091."
}
{
  "message": "Revert \"[SPARK-20941][SQL] Fix SubqueryExec Reuse\"\n\nThis reverts commit f7cf2096fdecb8edab61c8973c07c6fc877ee32d."
}
{
  "message": "[SPARK-21089][SQL] Fix DESC EXTENDED/FORMATTED to Show Table Properties\n\n### What changes were proposed in this pull request?\n\nSince both table properties and storage properties share the same key values, table properties are not shown in the output of DESC EXTENDED/FORMATTED when the storage properties are not empty.\n\nThis PR is to fix the above issue by renaming them to different keys.\n\n### How was this patch tested?\nAdded test cases.\n\nAuthor: Xiao Li <gatorsmile@gmail.com>\n\nCloses #18294 from gatorsmile/tableProperties."
}
{
  "message": "[SPARK-21085][SQL] Failed to read the partitioned table created by Spark 2.1\n\n### What changes were proposed in this pull request?\nBefore the PR, Spark is unable to read the partitioned table created by Spark 2.1 when the table schema does not put the partitioning column at the end of the schema.\n[assert(partitionFields.map(_.name) == partitionColumnNames)](https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/catalog/interface.scala#L234-L236)\n\nWhen reading the table metadata from the metastore, we also need to reorder the columns.\n\n### How was this patch tested?\nAdded test cases to check both Hive-serde and data source tables.\n\nAuthor: gatorsmile <gatorsmile@gmail.com>\n\nCloses #18295 from gatorsmile/reorderReadSchema."
}
{
  "message": "[SPARK-21057][ML] Do not use a PascalDistribution in countApprox\n\n## What changes were proposed in this pull request?\n\nUse Poisson analysis for approx count in all cases.\n\n## How was this patch tested?\n\nExisting tests.\n\nAuthor: Sean Owen <sowen@cloudera.com>\n\nCloses #18276 from srowen/SPARK-21057."
}
{
  "message": "[SPARK-20754][SQL][FOLLOWUP] Add Function Alias For MOD/POSITION.\n\n## What changes were proposed in this pull request?\n\nhttps://github.com/apache/spark/pull/18106 Support TRUNC (number),  We should also add function alias for `MOD `and `POSITION`.\n\n`POSITION(substr IN str) `is a synonym for `LOCATE(substr,str)`. same as MySQL: https://dev.mysql.com/doc/refman/5.7/en/string-functions.html#function_position\n\n## How was this patch tested?\n\nunit tests\n\nAuthor: Yuming Wang <wgyumg@gmail.com>\n\nCloses #18206 from wangyum/SPARK-20754-mod&position."
}
{
  "message": "[SPARK-19753][CORE] Un-register all shuffle output on a host in case of slave lost or fetch failure\n\n## What changes were proposed in this pull request?\n\nCurrently, when we detect fetch failure, we only remove the shuffle files produced by the executor, while the host itself might be down and all the shuffle files are not accessible. In case we are running multiple executors on a host, any host going down currently results in multiple fetch failures and multiple retries of the stage, which is very inefficient. If we remove all the shuffle files on that host, on first fetch failure, we can rerun all the tasks on that host in a single stage retry.\n\n## How was this patch tested?\n\nUnit testing and also ran a job on the cluster and made sure multiple retries are gone.\n\nAuthor: Sital Kedia <skedia@fb.com>\nAuthor: Imran Rashid <irashid@cloudera.com>\n\nCloses #18150 from sitalkedia/cleanup_shuffle."
}
{
  "message": "[SPARK-20986][SQL] Reset table's statistics after PruneFileSourcePartitions rule.\n\n## What changes were proposed in this pull request?\nAfter PruneFileSourcePartitions rule, It needs reset table's statistics because PruneFileSourcePartitions can filter some unnecessary partitions. So the statistics need to be changed.\n\n## How was this patch tested?\nadd unit test.\n\nAuthor: lianhuiwang <lianhuiwang09@gmail.com>\n\nCloses #18205 from lianhuiwang/SPARK-20986."
}
{
  "message": "[SPARK-12552][CORE] Correctly count the driver resource when recovering from failure for Master\n\nCurrently in Standalone HA mode, the resource usage of driver is not correctly counted in Master when recovering from failure, this will lead to some unexpected behaviors like negative value in UI.\n\nSo here fix this to also count the driver's resource usage.\n\nAlso changing the recovered app's state to `RUNNING` when fully recovered. Previously it will always be WAITING even fully recovered.\n\nandrewor14 please help to review, thanks a lot.\n\nAuthor: jerryshao <sshao@hortonworks.com>\n\nCloses #10506 from jerryshao/SPARK-12552."
}
{
  "message": "[SPARK-21016][CORE] Improve code fault tolerance for converting string to number\n\n## What changes were proposed in this pull request?\nWhen converting `string` to `number`(int, long or double),  if the string has a space before or after,will lead to unnecessary mistakes.\n\n## How was this patch tested?\nunit test\n\nAuthor: liuxian <liu.xian3@zte.com.cn>\n\nCloses #18238 from 10110346/lx-wip-0608."
}
{
  "message": "[SPARK-21051][SQL] Add hash map metrics to aggregate\n\n## What changes were proposed in this pull request?\n\nThis adds the average hash map probe metrics to hash aggregate.\n\n`BytesToBytesMap` already has API to get the metrics, this PR adds an API to `UnsafeFixedWidthAggregationMap` to access it.\n\nPreparing a test for this metrics seems tricky, because we don't know what collision keys are. For now, the test case generates random data large enough to have desired probe.\n\nTODO in later PR: add hash map metrics to join.\n\n## How was this patch tested?\n\nAdded test to SQLMetricsSuite.\n\nAuthor: Liang-Chi Hsieh <viirya@gmail.com>\n\nCloses #18258 from viirya/SPARK-20953."
}
{
  "message": "[SPARK-21064][CORE][TEST] Fix the default value bug in NettyBlockTransferServiceSuite\n\n## What changes were proposed in this pull request?\n\nThe default value for `spark.port.maxRetries` is 100,\nbut we use 10 in the suite file.\nSo we change it to 100 to avoid test failure.\n\n## How was this patch tested?\nNo test\n\nAuthor: DjvuLee <lihu@bytedance.com>\n\nCloses #18280 from djvulee/NettyTestBug."
}
{
  "message": "[SPARK-21060][WEB-UI] Css style about paging function is error in the executor page. Css style about paging function is error in the executor page. It is different of history server ui paging function css style.\n\n## What changes were proposed in this pull request?\n\nCss style about paging function is error in the executor page. It is different of history server ui paging function css style.\n\n**But their style should be consistent**. There are three reasons.\n\n1. The first reason: 'Previous', 'Next' and number should be the button format.\n\n2. The second reason: when you are on the first page, 'Previous' and '1' should be gray and can not be clicked.\n![1](https://user-images.githubusercontent.com/26266482/27026667-1fe745ee-4f91-11e7-8b34-150819d22bd3.png)\n\n3. The third reason: when you are on the last page, 'Previous' and 'Max number' should be gray and can not be clicked.\n![2](https://user-images.githubusercontent.com/26266482/27026811-9d8d6fa0-4f91-11e7-8b51-7816c3feb381.png)\n\nbefore fix:\n![fix_before](https://user-images.githubusercontent.com/26266482/27026428-47ec5c56-4f90-11e7-9dd5-d52c22d7bd36.png)\n\nafter fix:\n![fix_after](https://user-images.githubusercontent.com/26266482/27026439-50d17072-4f90-11e7-8405-6f81da5ab32c.png)\n\nThe style of history server ui:\n![history](https://user-images.githubusercontent.com/26266482/27026528-9c90f780-4f90-11e7-91e6-90d32651fe03.png)\n\n## How was this patch tested?\n\nmanual tests\n\nPlease review http://spark.apache.org/contributing.html before opening a pull request.\n\nAuthor: guoxiaolong <guo.xiaolong1@zte.com.cn>\nAuthor: 郭小龙 10207633 <guo.xiaolong1@zte.com.cn>\nAuthor: guoxiaolongzte <guo.xiaolong1@zte.com.cn>\n\nCloses #18275 from guoxiaolongzte/SPARK-21060."
}
{
  "message": "[SPARK-21039][SPARK CORE] Use treeAggregate instead of aggregate in DataFrame.stat.bloomFilter\n\n## What changes were proposed in this pull request?\nTo use treeAggregate instead of aggregate in DataFrame.stat.bloomFilter to parallelize the operation of merging the bloom filters\n(Please fill in changes proposed in this fix)\n\n## How was this patch tested?\nunit tests passed\n(Please explain how this patch was tested. E.g. unit tests, integration tests, manual tests)\n(If this patch involves UI changes, please attach a screenshot; otherwise, remove this)\n\nPlease review http://spark.apache.org/contributing.html before opening a pull request.\n\nAuthor: Rishabh Bhardwaj <rbnext29@gmail.com>\nAuthor: Rishabh Bhardwaj <admin@rishabh.local>\nAuthor: Rishabh Bhardwaj <r0b00ko@rishabh.Dlink>\nAuthor: Rishabh Bhardwaj <admin@Admins-MacBook-Pro.local>\nAuthor: Rishabh Bhardwaj <r0b00ko@rishabh.local>\n\nCloses #18263 from rishabhbhardwaj/SPARK-21039."
}
{
  "message": "[SPARK-21006][TESTS][FOLLOW-UP] Some Worker's RpcEnv is leaked in WorkerSuite\n\n## What changes were proposed in this pull request?\n\nCreate rpcEnv and run later needs shutdown. as #18226\n\n## How was this patch tested?\nunit test\n\nAuthor: liuxian <liu.xian3@zte.com.cn>\n\nCloses #18259 from 10110346/wip-lx-0610."
}
{
  "message": "[SPARK-20920][SQL] ForkJoinPool pools are leaked when writing hive tables with many partitions\n\n## What changes were proposed in this pull request?\n\nDon't leave thread pool running from AlterTableRecoverPartitionsCommand DDL command\n\n## How was this patch tested?\n\nExisting tests.\n\nAuthor: Sean Owen <sowen@cloudera.com>\n\nCloses #18216 from srowen/SPARK-20920."
}
{
  "message": "[TEST][SPARKR][CORE] Fix broken SparkSubmitSuite\n\n## What changes were proposed in this pull request?\n\nFix test file path. This is broken in #18264 and undetected since R-only changes don't build core and subsequent post-commit with the change built fine (again because it wasn't building core)\n\nactually appveyor builds everything but it's not running scala suites ...\n\n## How was this patch tested?\n\njenkins\nsrowen gatorsmile\n\nAuthor: Felix Cheung <felixcheung_m@hotmail.com>\n\nCloses #18283 from felixcheung/rsubmitsuite."
}
{
  "message": "[SPARK-19910][SQL] `stack` should not reject NULL values due to type mismatch\n\n## What changes were proposed in this pull request?\n\nSince `stack` function generates a table with nullable columns, it should allow mixed null values.\n\n```scala\nscala> sql(\"select stack(3, 1, 2, 3)\").printSchema\nroot\n |-- col0: integer (nullable = true)\n\nscala> sql(\"select stack(3, 1, 2, null)\").printSchema\norg.apache.spark.sql.AnalysisException: cannot resolve 'stack(3, 1, 2, NULL)' due to data type mismatch: Argument 1 (IntegerType) != Argument 3 (NullType); line 1 pos 7;\n```\n\n## How was this patch tested?\n\nPass the Jenkins with a new test case.\n\nAuthor: Dongjoon Hyun <dongjoon@apache.org>\n\nCloses #17251 from dongjoon-hyun/SPARK-19910."
}
{
  "message": "Revert \"[SPARK-21046][SQL] simplify the array offset and length in ColumnVector\"\n\nThis reverts commit 22dd65f58e12cb3a883d106fcccdff25a2a00fe8."
}
{
  "message": "[SPARK-20979][SS] Add RateSource to generate values for tests and benchmark\n\n## What changes were proposed in this pull request?\n\nThis PR adds RateSource for Structured Streaming so that the user can use it to generate data for tests and benchmark easily.\n\nThis source generates increment long values with timestamps. Each generated row has two columns: a timestamp column for the generated time and an auto increment long column starting with 0L.\n\nIt supports the following options:\n- `rowsPerSecond` (e.g. 100, default: 1): How many rows should be generated per second.\n- `rampUpTime` (e.g. 5s, default: 0s): How long to ramp up before the generating speed becomes `rowsPerSecond`. Using finer granularities than seconds will be truncated to integer seconds.\n- `numPartitions` (e.g. 10, default: Spark's default parallelism): The partition number for the generated rows. The source will try its best to reach `rowsPerSecond`, but the query may be resource constrained, and `numPartitions` can be tweaked to help reach the desired speed.\n\nHere is a simple example that prints 10 rows per seconds:\n```\n    spark.readStream\n      .format(\"rate\")\n      .option(\"rowsPerSecond\", \"10\")\n      .load()\n      .writeStream\n      .format(\"console\")\n      .start()\n```\n\nThe idea came from marmbrus and he did the initial work.\n\n## How was this patch tested?\n\nThe added tests.\n\nAuthor: Shixiong Zhu <shixiong@databricks.com>\nAuthor: Michael Armbrust <michael@databricks.com>\n\nCloses #18199 from zsxwing/rate."
}
{
  "message": "[SPARK-21050][ML] Word2vec persistence overflow bug fix\n\n## What changes were proposed in this pull request?\n\nThe method calculateNumberOfPartitions() uses Int, not Long (unlike the MLlib version), so it is very easily to have an overflow in calculating the number of partitions for ML persistence.\n\nThis modifies the calculations to use Long.\n\n## How was this patch tested?\n\nNew unit test.  I verified that the test fails before this patch.\n\nAuthor: Joseph K. Bradley <joseph@databricks.com>\n\nCloses #18265 from jkbradley/word2vec-save-fix."
}
{
  "message": "[SPARK-21059][SQL] LikeSimplification can NPE on null pattern\n\n## What changes were proposed in this pull request?\nThis patch fixes a bug that can cause NullPointerException in LikeSimplification, when the pattern for like is null.\n\n## How was this patch tested?\nAdded a new unit test case in LikeSimplificationSuite.\n\nAuthor: Reynold Xin <rxin@databricks.com>\n\nCloses #18273 from rxin/SPARK-21059."
}
{
  "message": "[SPARK-20345][SQL] Fix STS error handling logic on HiveSQLException\n\n## What changes were proposed in this pull request?\n\n[SPARK-5100](https://github.com/apache/spark/commit/343d3bfafd449a0371feb6a88f78e07302fa7143) added Spark Thrift Server(STS) UI and the following logic to handle exceptions on case `Throwable`.\n\n```scala\nHiveThriftServer2.listener.onStatementError(\n  statementId, e.getMessage, SparkUtils.exceptionString(e))\n```\n\nHowever, there occurred a missed case after implementing [SPARK-6964](https://github.com/apache/spark/commit/eb19d3f75cbd002f7e72ce02017a8de67f562792)'s `Support Cancellation in the Thrift Server` by adding case `HiveSQLException` before case `Throwable`.\n\n```scala\ncase e: HiveSQLException =>\n  if (getStatus().getState() == OperationState.CANCELED) {\n    return\n  } else {\n    setState(OperationState.ERROR)\n    throw e\n  }\n  // Actually do need to catch Throwable as some failures don't inherit from Exception and\n  // HiveServer will silently swallow them.\ncase e: Throwable =>\n  val currentState = getStatus().getState()\n  logError(s\"Error executing query, currentState $currentState, \", e)\n  setState(OperationState.ERROR)\n  HiveThriftServer2.listener.onStatementError(\n    statementId, e.getMessage, SparkUtils.exceptionString(e))\n  throw new HiveSQLException(e.toString)\n```\n\nLogically, we had better add `HiveThriftServer2.listener.onStatementError` on case `HiveSQLException`, too.\n\n## How was this patch tested?\n\nN/A\n\nAuthor: Dongjoon Hyun <dongjoon@apache.org>\n\nCloses #17643 from dongjoon-hyun/SPARK-20345."
}
{
  "message": "[SPARK-17914][SQL] Fix parsing of timestamp strings with nanoseconds\n\nThe PR contains a tiny change to fix the way Spark parses string literals into timestamps. Currently, some timestamps that contain nanoseconds are corrupted during the conversion from internal UTF8Strings into the internal representation of timestamps.\n\nConsider the following example:\n```\nspark.sql(\"SELECT cast('2015-01-02 00:00:00.000000001' as TIMESTAMP)\").show(false)\n+------------------------------------------------+\n|CAST(2015-01-02 00:00:00.000000001 AS TIMESTAMP)|\n+------------------------------------------------+\n|2015-01-02 00:00:00.000001                      |\n+------------------------------------------------+\n```\n\nThe fix was tested with existing tests. Also, there is a new test to cover cases that did not work previously.\n\nAuthor: aokolnychyi <anton.okolnychyi@sap.com>\n\nCloses #18252 from aokolnychyi/spark-17914."
}
{
  "message": "[SPARK-21046][SQL] simplify the array offset and length in ColumnVector\n\n## What changes were proposed in this pull request?\n\nCurrently when a `ColumnVector` stores array type elements, we will use 2 arrays for lengths and offsets and implement them individually in on-heap and off-heap column vector.\n\nIn this PR, we use one array to represent both offsets and lengths, so that we can treat it as `ColumnVector` and all the logic can go to the base class `ColumnVector`\n\n## How was this patch tested?\n\nexisting tests.\n\nAuthor: Wenchen Fan <wenchen@databricks.com>\n\nCloses #18260 from cloud-fan/put."
}
{
  "message": "[SPARK-21041][SQL] SparkSession.range should be consistent with SparkContext.range\n\n## What changes were proposed in this pull request?\n\nThis PR fixes the inconsistency in `SparkSession.range`.\n\n**BEFORE**\n```scala\nscala> spark.range(java.lang.Long.MAX_VALUE - 3, java.lang.Long.MIN_VALUE + 2, 1).collect\nres2: Array[Long] = Array(9223372036854775804, 9223372036854775805, 9223372036854775806)\n```\n\n**AFTER**\n```scala\nscala> spark.range(java.lang.Long.MAX_VALUE - 3, java.lang.Long.MIN_VALUE + 2, 1).collect\nres2: Array[Long] = Array()\n```\n\n## How was this patch tested?\n\nPass the Jenkins with newly added test cases.\n\nAuthor: Dongjoon Hyun <dongjoon@apache.org>\n\nCloses #18257 from dongjoon-hyun/SPARK-21041."
}
{
  "message": "[DOCS] Fix error: ambiguous reference to overloaded definition\n\n## What changes were proposed in this pull request?\n\n`df.groupBy.count()` should be `df.groupBy().count()` , otherwise there is an error :\n\nambiguous reference to overloaded definition, both method groupBy in class Dataset of type (col1: String, cols: String*) and method groupBy in class Dataset of type (cols: org.apache.spark.sql.Column*)\n\n## How was this patch tested?\n\n```scala\nval df = spark.readStream.schema(...).json(...)\nval dfCounts = df.groupBy().count()\n```\n\nAuthor: Ziyue Huang <zyhuang94@gmail.com>\n\nCloses #18272 from ZiyueHuang/master."
}
{
  "message": "[SPARK-20665][SQL][FOLLOW-UP] Move test case to MathExpressionsSuite\n\n## What changes were proposed in this pull request?\n\n add test case to MathExpressionsSuite as #17906\n\n## How was this patch tested?\n\nunit test cases\n\nAuthor: liuxian <liu.xian3@zte.com.cn>\n\nCloses #18082 from 10110346/wip-lx-0524."
}
{
  "message": "[SPARK-20715] Store MapStatuses only in MapOutputTracker, not ShuffleMapStage\n\n## What changes were proposed in this pull request?\n\nThis PR refactors `ShuffleMapStage` and `MapOutputTracker` in order to simplify the management of `MapStatuses`, reduce driver memory consumption, and remove a potential source of scheduler correctness bugs.\n\n### Background\n\nIn Spark there are currently two places where MapStatuses are tracked:\n\n- The `MapOutputTracker` maintains an `Array[MapStatus]` storing a single location for each map output. This mapping is used by the `DAGScheduler` for determining reduce-task locality preferences (when locality-aware reduce task scheduling is enabled) and is also used to serve map output locations to executors / tasks.\n- Each `ShuffleMapStage` also contains a mapping of `Array[List[MapStatus]]` which holds the complete set of locations where each map output could be available. This mapping is used to determine which map tasks need to be run when constructing `TaskSets` for the stage.\n\nThis duplication adds complexity and creates the potential for certain types of correctness bugs.  Bad things can happen if these two copies of the map output locations get out of sync. For instance, if the `MapOutputTracker` is missing locations for a map output but `ShuffleMapStage` believes that locations are available then tasks will fail with `MetadataFetchFailedException` but `ShuffleMapStage` will not be updated to reflect the missing map outputs, leading to situations where the stage will be reattempted (because downstream stages experienced fetch failures) but no task sets will be launched (because `ShuffleMapStage` thinks all maps are available).\n\nI observed this behavior in a real-world deployment. I'm still not quite sure how the state got out of sync in the first place, but we can completely avoid this class of bug if we eliminate the duplicate state.\n\n### Why we only need to track a single location for each map output\n\nI think that storing an `Array[List[MapStatus]]` in `ShuffleMapStage` is unnecessary.\n\nFirst, note that this adds memory/object bloat to the driver we need one extra `List` per task. If you have millions of tasks across all stages then this can add up to be a significant amount of resources.\n\nSecondly, I believe that it's extremely uncommon that these lists will ever contain more than one entry. It's not impossible, but is very unlikely given the conditions which must occur for that to happen:\n\n- In normal operation (no task failures) we'll only run each task once and thus will have at most one output.\n- If speculation is enabled then it's possible that we'll have multiple attempts of a task. The TaskSetManager will [kill duplicate attempts of a task](https://github.com/apache/spark/blob/04901dd03a3f8062fd39ea38d585935ff71a9248/core/src/main/scala/org/apache/spark/scheduler/TaskSetManager.scala#L717) after a task finishes successfully, reducing the likelihood that both the original and speculated task will successfully register map outputs.\n- There is a [comment in `TaskSetManager`](https://github.com/apache/spark/blob/04901dd03a3f8062fd39ea38d585935ff71a9248/core/src/main/scala/org/apache/spark/scheduler/TaskSetManager.scala#L113) which suggests that running tasks are not killed if a task set becomes a zombie. However:\n  - If the task set becomes a zombie due to the job being cancelled then it doesn't matter whether we record map outputs.\n  - If the task set became a zombie because of a stage failure (e.g. the map stage itself had a fetch failure from an upstream match stage) then I believe that the \"failedEpoch\" will be updated which may cause map outputs from still-running tasks to [be ignored](https://github.com/apache/spark/blob/04901dd03a3f8062fd39ea38d585935ff71a9248/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala#L1213). (I'm not 100% sure on this point, though).\n- Even if you _do_ manage to record multiple map outputs for a stage, only a single map output is reported to / tracked by the MapOutputTracker. The only situation where the additional output locations could actually be read or used would be if a task experienced a `FetchFailure` exception. The most likely cause of a `FetchFailure` exception is an executor lost, which will have most likely caused the loss of several map tasks' output, so saving on potential re-execution of a single map task isn't a huge win if we're going to have to recompute several other lost map outputs from other tasks which ran on that lost executor. Also note that the re-population of MapOutputTracker state from state in the ShuffleMapTask only happens after the reduce stage has failed; the additional location doesn't help to prevent FetchFailures but, instead, can only reduce the amount of work when recomputing missing parent stages.\n\nGiven this, this patch chooses to do away with tracking multiple locations for map outputs and instead stores only a single location. This change removes the main distinction between the `ShuffleMapTask` and `MapOutputTracker`'s copies of this state, paving the way for storing it only in the `MapOutputTracker`.\n\n### Overview of other changes\n\n- Significantly simplified the cache / lock management inside of the `MapOutputTrackerMaster`:\n  - The old code had several parallel `HashMap`s which had to be guarded by maps of `Object`s which were used as locks. This code was somewhat complicated to follow.\n  - The new code uses a new `ShuffleStatus` class to group together all of the state associated with a particular shuffle, including cached serialized map statuses, significantly simplifying the logic.\n- Moved more code out of the shared `MapOutputTracker` abstract base class and into the `MapOutputTrackerMaster` and `MapOutputTrackerWorker` subclasses. This makes it easier to reason about which functionality needs to be supported only on the driver or executor.\n- Removed a bunch of code from the `DAGScheduler` which was used to synchronize information from the `MapOutputTracker` to `ShuffleMapStage`.\n- Added comments to clarify the role of `MapOutputTrackerMaster`'s `epoch` in invalidating executor-side shuffle map output caches.\n\nI will comment on these changes via inline GitHub review comments.\n\n/cc hvanhovell and rxin (whom I discussed this with offline), tgravescs (who recently worked on caching of serialized MapOutputStatuses), and kayousterhout and markhamstra (for scheduler changes).\n\n## How was this patch tested?\n\nExisting tests. I purposely avoided making interface / API which would require significant updates or modifications to test code.\n\nAuthor: Josh Rosen <joshrosen@databricks.com>\n\nCloses #17955 from JoshRosen/map-output-tracker-rewrite."
}
{
  "message": "[SPARK-18891][SQL] Support for specific Java List subtypes\n\n## What changes were proposed in this pull request?\n\nAdd support for specific Java `List` subtypes in deserialization as well as a generic implicit encoder.\n\nAll `List` subtypes are supported by using either the size-specifying constructor (one `int` parameter) or the default constructor.\n\nInterfaces/abstract classes use the following implementations:\n\n* `java.util.List`, `java.util.AbstractList` or `java.util.AbstractSequentialList` => `java.util.ArrayList`\n\n## How was this patch tested?\n\n```bash\nbuild/mvn -DskipTests clean package && dev/run-tests\n```\n\nAdditionally in Spark shell:\n\n```\nscala> val jlist = new java.util.LinkedList[Int]; jlist.add(1)\njlist: java.util.LinkedList[Int] = [1]\nres0: Boolean = true\n\nscala> Seq(jlist).toDS().map(_.element()).collect()\nres1: Array[Int] = Array(1)\n```\n\nAuthor: Michal Senkyr <mike.senkyr@gmail.com>\n\nCloses #18009 from michalsenkyr/dataset-java-lists."
}
{
  "message": "[SPARK-18891][SQL] Support for Scala Map collection types\n\n## What changes were proposed in this pull request?\n\nAdd support for arbitrary Scala `Map` types in deserialization as well as a generic implicit encoder.\n\nUsed the builder approach as in #16541 to construct any provided `Map` type upon deserialization.\n\nPlease note that this PR also adds (ignored) tests for issue [SPARK-19104 CompileException with Map and Case Class in Spark 2.1.0](https://issues.apache.org/jira/browse/SPARK-19104) but doesn't solve it.\n\nAdded support for Java Maps in codegen code (encoders will be added in a different PR) with the following default implementations for interfaces/abstract classes:\n\n* `java.util.Map`, `java.util.AbstractMap` => `java.util.HashMap`\n* `java.util.SortedMap`, `java.util.NavigableMap` => `java.util.TreeMap`\n* `java.util.concurrent.ConcurrentMap` => `java.util.concurrent.ConcurrentHashMap`\n* `java.util.concurrent.ConcurrentNavigableMap` => `java.util.concurrent.ConcurrentSkipListMap`\n\nResulting codegen for `Seq(Map(1 -> 2)).toDS().map(identity).queryExecution.debug.codegen`:\n\n```\n/* 001 */ public Object generate(Object[] references) {\n/* 002 */   return new GeneratedIterator(references);\n/* 003 */ }\n/* 004 */\n/* 005 */ final class GeneratedIterator extends org.apache.spark.sql.execution.BufferedRowIterator {\n/* 006 */   private Object[] references;\n/* 007 */   private scala.collection.Iterator[] inputs;\n/* 008 */   private scala.collection.Iterator inputadapter_input;\n/* 009 */   private boolean CollectObjectsToMap_loopIsNull1;\n/* 010 */   private int CollectObjectsToMap_loopValue0;\n/* 011 */   private boolean CollectObjectsToMap_loopIsNull3;\n/* 012 */   private int CollectObjectsToMap_loopValue2;\n/* 013 */   private UnsafeRow deserializetoobject_result;\n/* 014 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder deserializetoobject_holder;\n/* 015 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter deserializetoobject_rowWriter;\n/* 016 */   private scala.collection.immutable.Map mapelements_argValue;\n/* 017 */   private UnsafeRow mapelements_result;\n/* 018 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder mapelements_holder;\n/* 019 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter mapelements_rowWriter;\n/* 020 */   private UnsafeRow serializefromobject_result;\n/* 021 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder serializefromobject_holder;\n/* 022 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter serializefromobject_rowWriter;\n/* 023 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeArrayWriter serializefromobject_arrayWriter;\n/* 024 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeArrayWriter serializefromobject_arrayWriter1;\n/* 025 */\n/* 026 */   public GeneratedIterator(Object[] references) {\n/* 027 */     this.references = references;\n/* 028 */   }\n/* 029 */\n/* 030 */   public void init(int index, scala.collection.Iterator[] inputs) {\n/* 031 */     partitionIndex = index;\n/* 032 */     this.inputs = inputs;\n/* 033 */     wholestagecodegen_init_0();\n/* 034 */     wholestagecodegen_init_1();\n/* 035 */\n/* 036 */   }\n/* 037 */\n/* 038 */   private void wholestagecodegen_init_0() {\n/* 039 */     inputadapter_input = inputs[0];\n/* 040 */\n/* 041 */     deserializetoobject_result = new UnsafeRow(1);\n/* 042 */     this.deserializetoobject_holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(deserializetoobject_result, 32);\n/* 043 */     this.deserializetoobject_rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(deserializetoobject_holder, 1);\n/* 044 */\n/* 045 */     mapelements_result = new UnsafeRow(1);\n/* 046 */     this.mapelements_holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(mapelements_result, 32);\n/* 047 */     this.mapelements_rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(mapelements_holder, 1);\n/* 048 */     serializefromobject_result = new UnsafeRow(1);\n/* 049 */     this.serializefromobject_holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(serializefromobject_result, 32);\n/* 050 */     this.serializefromobject_rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(serializefromobject_holder, 1);\n/* 051 */     this.serializefromobject_arrayWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeArrayWriter();\n/* 052 */\n/* 053 */   }\n/* 054 */\n/* 055 */   private void wholestagecodegen_init_1() {\n/* 056 */     this.serializefromobject_arrayWriter1 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeArrayWriter();\n/* 057 */\n/* 058 */   }\n/* 059 */\n/* 060 */   protected void processNext() throws java.io.IOException {\n/* 061 */     while (inputadapter_input.hasNext() && !stopEarly()) {\n/* 062 */       InternalRow inputadapter_row = (InternalRow) inputadapter_input.next();\n/* 063 */       boolean inputadapter_isNull = inputadapter_row.isNullAt(0);\n/* 064 */       MapData inputadapter_value = inputadapter_isNull ? null : (inputadapter_row.getMap(0));\n/* 065 */\n/* 066 */       boolean deserializetoobject_isNull1 = true;\n/* 067 */       ArrayData deserializetoobject_value1 = null;\n/* 068 */       if (!inputadapter_isNull) {\n/* 069 */         deserializetoobject_isNull1 = false;\n/* 070 */         if (!deserializetoobject_isNull1) {\n/* 071 */           Object deserializetoobject_funcResult = null;\n/* 072 */           deserializetoobject_funcResult = inputadapter_value.keyArray();\n/* 073 */           if (deserializetoobject_funcResult == null) {\n/* 074 */             deserializetoobject_isNull1 = true;\n/* 075 */           } else {\n/* 076 */             deserializetoobject_value1 = (ArrayData) deserializetoobject_funcResult;\n/* 077 */           }\n/* 078 */\n/* 079 */         }\n/* 080 */         deserializetoobject_isNull1 = deserializetoobject_value1 == null;\n/* 081 */       }\n/* 082 */\n/* 083 */       boolean deserializetoobject_isNull3 = true;\n/* 084 */       ArrayData deserializetoobject_value3 = null;\n/* 085 */       if (!inputadapter_isNull) {\n/* 086 */         deserializetoobject_isNull3 = false;\n/* 087 */         if (!deserializetoobject_isNull3) {\n/* 088 */           Object deserializetoobject_funcResult1 = null;\n/* 089 */           deserializetoobject_funcResult1 = inputadapter_value.valueArray();\n/* 090 */           if (deserializetoobject_funcResult1 == null) {\n/* 091 */             deserializetoobject_isNull3 = true;\n/* 092 */           } else {\n/* 093 */             deserializetoobject_value3 = (ArrayData) deserializetoobject_funcResult1;\n/* 094 */           }\n/* 095 */\n/* 096 */         }\n/* 097 */         deserializetoobject_isNull3 = deserializetoobject_value3 == null;\n/* 098 */       }\n/* 099 */       scala.collection.immutable.Map deserializetoobject_value = null;\n/* 100 */\n/* 101 */       if ((deserializetoobject_isNull1 && !deserializetoobject_isNull3) ||\n/* 102 */         (!deserializetoobject_isNull1 && deserializetoobject_isNull3)) {\n/* 103 */         throw new RuntimeException(\"Invalid state: Inconsistent nullability of key-value\");\n/* 104 */       }\n/* 105 */\n/* 106 */       if (!deserializetoobject_isNull1) {\n/* 107 */         if (deserializetoobject_value1.numElements() != deserializetoobject_value3.numElements()) {\n/* 108 */           throw new RuntimeException(\"Invalid state: Inconsistent lengths of key-value arrays\");\n/* 109 */         }\n/* 110 */         int deserializetoobject_dataLength = deserializetoobject_value1.numElements();\n/* 111 */\n/* 112 */         scala.collection.mutable.Builder CollectObjectsToMap_builderValue5 = scala.collection.immutable.Map$.MODULE$.newBuilder();\n/* 113 */         CollectObjectsToMap_builderValue5.sizeHint(deserializetoobject_dataLength);\n/* 114 */\n/* 115 */         int deserializetoobject_loopIndex = 0;\n/* 116 */         while (deserializetoobject_loopIndex < deserializetoobject_dataLength) {\n/* 117 */           CollectObjectsToMap_loopValue0 = (int) (deserializetoobject_value1.getInt(deserializetoobject_loopIndex));\n/* 118 */           CollectObjectsToMap_loopValue2 = (int) (deserializetoobject_value3.getInt(deserializetoobject_loopIndex));\n/* 119 */           CollectObjectsToMap_loopIsNull1 = deserializetoobject_value1.isNullAt(deserializetoobject_loopIndex);\n/* 120 */           CollectObjectsToMap_loopIsNull3 = deserializetoobject_value3.isNullAt(deserializetoobject_loopIndex);\n/* 121 */\n/* 122 */           if (CollectObjectsToMap_loopIsNull1) {\n/* 123 */             throw new RuntimeException(\"Found null in map key!\");\n/* 124 */           }\n/* 125 */\n/* 126 */           scala.Tuple2 CollectObjectsToMap_loopValue4;\n/* 127 */\n/* 128 */           if (CollectObjectsToMap_loopIsNull3) {\n/* 129 */             CollectObjectsToMap_loopValue4 = new scala.Tuple2(CollectObjectsToMap_loopValue0, null);\n/* 130 */           } else {\n/* 131 */             CollectObjectsToMap_loopValue4 = new scala.Tuple2(CollectObjectsToMap_loopValue0, CollectObjectsToMap_loopValue2);\n/* 132 */           }\n/* 133 */\n/* 134 */           CollectObjectsToMap_builderValue5.$plus$eq(CollectObjectsToMap_loopValue4);\n/* 135 */\n/* 136 */           deserializetoobject_loopIndex += 1;\n/* 137 */         }\n/* 138 */\n/* 139 */         deserializetoobject_value = (scala.collection.immutable.Map) CollectObjectsToMap_builderValue5.result();\n/* 140 */       }\n/* 141 */\n/* 142 */       boolean mapelements_isNull = true;\n/* 143 */       scala.collection.immutable.Map mapelements_value = null;\n/* 144 */       if (!false) {\n/* 145 */         mapelements_argValue = deserializetoobject_value;\n/* 146 */\n/* 147 */         mapelements_isNull = false;\n/* 148 */         if (!mapelements_isNull) {\n/* 149 */           Object mapelements_funcResult = null;\n/* 150 */           mapelements_funcResult = ((scala.Function1) references[0]).apply(mapelements_argValue);\n/* 151 */           if (mapelements_funcResult == null) {\n/* 152 */             mapelements_isNull = true;\n/* 153 */           } else {\n/* 154 */             mapelements_value = (scala.collection.immutable.Map) mapelements_funcResult;\n/* 155 */           }\n/* 156 */\n/* 157 */         }\n/* 158 */         mapelements_isNull = mapelements_value == null;\n/* 159 */       }\n/* 160 */\n/* 161 */       MapData serializefromobject_value = null;\n/* 162 */       if (!mapelements_isNull) {\n/* 163 */         final int serializefromobject_length = mapelements_value.size();\n/* 164 */         final Object[] serializefromobject_convertedKeys = new Object[serializefromobject_length];\n/* 165 */         final Object[] serializefromobject_convertedValues = new Object[serializefromobject_length];\n/* 166 */         int serializefromobject_index = 0;\n/* 167 */         final scala.collection.Iterator serializefromobject_entries = mapelements_value.iterator();\n/* 168 */         while(serializefromobject_entries.hasNext()) {\n/* 169 */           final scala.Tuple2 serializefromobject_entry = (scala.Tuple2) serializefromobject_entries.next();\n/* 170 */           int ExternalMapToCatalyst_key1 = (Integer) serializefromobject_entry._1();\n/* 171 */           int ExternalMapToCatalyst_value1 = (Integer) serializefromobject_entry._2();\n/* 172 */\n/* 173 */           boolean ExternalMapToCatalyst_value_isNull1 = false;\n/* 174 */\n/* 175 */           if (false) {\n/* 176 */             throw new RuntimeException(\"Cannot use null as map key!\");\n/* 177 */           } else {\n/* 178 */             serializefromobject_convertedKeys[serializefromobject_index] = (Integer) ExternalMapToCatalyst_key1;\n/* 179 */           }\n/* 180 */\n/* 181 */           if (false) {\n/* 182 */             serializefromobject_convertedValues[serializefromobject_index] = null;\n/* 183 */           } else {\n/* 184 */             serializefromobject_convertedValues[serializefromobject_index] = (Integer) ExternalMapToCatalyst_value1;\n/* 185 */           }\n/* 186 */\n/* 187 */           serializefromobject_index++;\n/* 188 */         }\n/* 189 */\n/* 190 */         serializefromobject_value = new org.apache.spark.sql.catalyst.util.ArrayBasedMapData(new org.apache.spark.sql.catalyst.util.GenericArrayData(serializefromobject_convertedKeys), new org.apache.spark.sql.catalyst.util.GenericArrayData(serializefromobject_convertedValues));\n/* 191 */       }\n/* 192 */       serializefromobject_holder.reset();\n/* 193 */\n/* 194 */       serializefromobject_rowWriter.zeroOutNullBytes();\n/* 195 */\n/* 196 */       if (mapelements_isNull) {\n/* 197 */         serializefromobject_rowWriter.setNullAt(0);\n/* 198 */       } else {\n/* 199 */         // Remember the current cursor so that we can calculate how many bytes are\n/* 200 */         // written later.\n/* 201 */         final int serializefromobject_tmpCursor = serializefromobject_holder.cursor;\n/* 202 */\n/* 203 */         if (serializefromobject_value instanceof UnsafeMapData) {\n/* 204 */           final int serializefromobject_sizeInBytes = ((UnsafeMapData) serializefromobject_value).getSizeInBytes();\n/* 205 */           // grow the global buffer before writing data.\n/* 206 */           serializefromobject_holder.grow(serializefromobject_sizeInBytes);\n/* 207 */           ((UnsafeMapData) serializefromobject_value).writeToMemory(serializefromobject_holder.buffer, serializefromobject_holder.cursor);\n/* 208 */           serializefromobject_holder.cursor += serializefromobject_sizeInBytes;\n/* 209 */\n/* 210 */         } else {\n/* 211 */           final ArrayData serializefromobject_keys = serializefromobject_value.keyArray();\n/* 212 */           final ArrayData serializefromobject_values = serializefromobject_value.valueArray();\n/* 213 */\n/* 214 */           // preserve 8 bytes to write the key array numBytes later.\n/* 215 */           serializefromobject_holder.grow(8);\n/* 216 */           serializefromobject_holder.cursor += 8;\n/* 217 */\n/* 218 */           // Remember the current cursor so that we can write numBytes of key array later.\n/* 219 */           final int serializefromobject_tmpCursor1 = serializefromobject_holder.cursor;\n/* 220 */\n/* 221 */           if (serializefromobject_keys instanceof UnsafeArrayData) {\n/* 222 */             final int serializefromobject_sizeInBytes1 = ((UnsafeArrayData) serializefromobject_keys).getSizeInBytes();\n/* 223 */             // grow the global buffer before writing data.\n/* 224 */             serializefromobject_holder.grow(serializefromobject_sizeInBytes1);\n/* 225 */             ((UnsafeArrayData) serializefromobject_keys).writeToMemory(serializefromobject_holder.buffer, serializefromobject_holder.cursor);\n/* 226 */             serializefromobject_holder.cursor += serializefromobject_sizeInBytes1;\n/* 227 */\n/* 228 */           } else {\n/* 229 */             final int serializefromobject_numElements = serializefromobject_keys.numElements();\n/* 230 */             serializefromobject_arrayWriter.initialize(serializefromobject_holder, serializefromobject_numElements, 4);\n/* 231 */\n/* 232 */             for (int serializefromobject_index1 = 0; serializefromobject_index1 < serializefromobject_numElements; serializefromobject_index1++) {\n/* 233 */               if (serializefromobject_keys.isNullAt(serializefromobject_index1)) {\n/* 234 */                 serializefromobject_arrayWriter.setNullInt(serializefromobject_index1);\n/* 235 */               } else {\n/* 236 */                 final int serializefromobject_element = serializefromobject_keys.getInt(serializefromobject_index1);\n/* 237 */                 serializefromobject_arrayWriter.write(serializefromobject_index1, serializefromobject_element);\n/* 238 */               }\n/* 239 */             }\n/* 240 */           }\n/* 241 */\n/* 242 */           // Write the numBytes of key array into the first 8 bytes.\n/* 243 */           Platform.putLong(serializefromobject_holder.buffer, serializefromobject_tmpCursor1 - 8, serializefromobject_holder.cursor - serializefromobject_tmpCursor1);\n/* 244 */\n/* 245 */           if (serializefromobject_values instanceof UnsafeArrayData) {\n/* 246 */             final int serializefromobject_sizeInBytes2 = ((UnsafeArrayData) serializefromobject_values).getSizeInBytes();\n/* 247 */             // grow the global buffer before writing data.\n/* 248 */             serializefromobject_holder.grow(serializefromobject_sizeInBytes2);\n/* 249 */             ((UnsafeArrayData) serializefromobject_values).writeToMemory(serializefromobject_holder.buffer, serializefromobject_holder.cursor);\n/* 250 */             serializefromobject_holder.cursor += serializefromobject_sizeInBytes2;\n/* 251 */\n/* 252 */           } else {\n/* 253 */             final int serializefromobject_numElements1 = serializefromobject_values.numElements();\n/* 254 */             serializefromobject_arrayWriter1.initialize(serializefromobject_holder, serializefromobject_numElements1, 4);\n/* 255 */\n/* 256 */             for (int serializefromobject_index2 = 0; serializefromobject_index2 < serializefromobject_numElements1; serializefromobject_index2++) {\n/* 257 */               if (serializefromobject_values.isNullAt(serializefromobject_index2)) {\n/* 258 */                 serializefromobject_arrayWriter1.setNullInt(serializefromobject_index2);\n/* 259 */               } else {\n/* 260 */                 final int serializefromobject_element1 = serializefromobject_values.getInt(serializefromobject_index2);\n/* 261 */                 serializefromobject_arrayWriter1.write(serializefromobject_index2, serializefromobject_element1);\n/* 262 */               }\n/* 263 */             }\n/* 264 */           }\n/* 265 */\n/* 266 */         }\n/* 267 */\n/* 268 */         serializefromobject_rowWriter.setOffsetAndSize(0, serializefromobject_tmpCursor, serializefromobject_holder.cursor - serializefromobject_tmpCursor);\n/* 269 */       }\n/* 270 */       serializefromobject_result.setTotalSize(serializefromobject_holder.totalSize());\n/* 271 */       append(serializefromobject_result);\n/* 272 */       if (shouldStop()) return;\n/* 273 */     }\n/* 274 */   }\n/* 275 */ }\n```\n\nCodegen for `java.util.Map`:\n\n```\n/* 001 */ public Object generate(Object[] references) {\n/* 002 */   return new GeneratedIterator(references);\n/* 003 */ }\n/* 004 */\n/* 005 */ final class GeneratedIterator extends org.apache.spark.sql.execution.BufferedRowIterator {\n/* 006 */   private Object[] references;\n/* 007 */   private scala.collection.Iterator[] inputs;\n/* 008 */   private scala.collection.Iterator inputadapter_input;\n/* 009 */   private boolean CollectObjectsToMap_loopIsNull1;\n/* 010 */   private int CollectObjectsToMap_loopValue0;\n/* 011 */   private boolean CollectObjectsToMap_loopIsNull3;\n/* 012 */   private int CollectObjectsToMap_loopValue2;\n/* 013 */   private UnsafeRow deserializetoobject_result;\n/* 014 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder deserializetoobject_holder;\n/* 015 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter deserializetoobject_rowWriter;\n/* 016 */   private java.util.HashMap mapelements_argValue;\n/* 017 */   private UnsafeRow mapelements_result;\n/* 018 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder mapelements_holder;\n/* 019 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter mapelements_rowWriter;\n/* 020 */   private UnsafeRow serializefromobject_result;\n/* 021 */   private org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder serializefromobject_holder;\n/* 022 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter serializefromobject_rowWriter;\n/* 023 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeArrayWriter serializefromobject_arrayWriter;\n/* 024 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeArrayWriter serializefromobject_arrayWriter1;\n/* 025 */\n/* 026 */   public GeneratedIterator(Object[] references) {\n/* 027 */     this.references = references;\n/* 028 */   }\n/* 029 */\n/* 030 */   public void init(int index, scala.collection.Iterator[] inputs) {\n/* 031 */     partitionIndex = index;\n/* 032 */     this.inputs = inputs;\n/* 033 */     wholestagecodegen_init_0();\n/* 034 */     wholestagecodegen_init_1();\n/* 035 */\n/* 036 */   }\n/* 037 */\n/* 038 */   private void wholestagecodegen_init_0() {\n/* 039 */     inputadapter_input = inputs[0];\n/* 040 */\n/* 041 */     deserializetoobject_result = new UnsafeRow(1);\n/* 042 */     this.deserializetoobject_holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(deserializetoobject_result, 32);\n/* 043 */     this.deserializetoobject_rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(deserializetoobject_holder, 1);\n/* 044 */\n/* 045 */     mapelements_result = new UnsafeRow(1);\n/* 046 */     this.mapelements_holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(mapelements_result, 32);\n/* 047 */     this.mapelements_rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(mapelements_holder, 1);\n/* 048 */     serializefromobject_result = new UnsafeRow(1);\n/* 049 */     this.serializefromobject_holder = new org.apache.spark.sql.catalyst.expressions.codegen.BufferHolder(serializefromobject_result, 32);\n/* 050 */     this.serializefromobject_rowWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(serializefromobject_holder, 1);\n/* 051 */     this.serializefromobject_arrayWriter = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeArrayWriter();\n/* 052 */\n/* 053 */   }\n/* 054 */\n/* 055 */   private void wholestagecodegen_init_1() {\n/* 056 */     this.serializefromobject_arrayWriter1 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeArrayWriter();\n/* 057 */\n/* 058 */   }\n/* 059 */\n/* 060 */   protected void processNext() throws java.io.IOException {\n/* 061 */     while (inputadapter_input.hasNext() && !stopEarly()) {\n/* 062 */       InternalRow inputadapter_row = (InternalRow) inputadapter_input.next();\n/* 063 */       boolean inputadapter_isNull = inputadapter_row.isNullAt(0);\n/* 064 */       MapData inputadapter_value = inputadapter_isNull ? null : (inputadapter_row.getMap(0));\n/* 065 */\n/* 066 */       boolean deserializetoobject_isNull1 = true;\n/* 067 */       ArrayData deserializetoobject_value1 = null;\n/* 068 */       if (!inputadapter_isNull) {\n/* 069 */         deserializetoobject_isNull1 = false;\n/* 070 */         if (!deserializetoobject_isNull1) {\n/* 071 */           Object deserializetoobject_funcResult = null;\n/* 072 */           deserializetoobject_funcResult = inputadapter_value.keyArray();\n/* 073 */           if (deserializetoobject_funcResult == null) {\n/* 074 */             deserializetoobject_isNull1 = true;\n/* 075 */           } else {\n/* 076 */             deserializetoobject_value1 = (ArrayData) deserializetoobject_funcResult;\n/* 077 */           }\n/* 078 */\n/* 079 */         }\n/* 080 */         deserializetoobject_isNull1 = deserializetoobject_value1 == null;\n/* 081 */       }\n/* 082 */\n/* 083 */       boolean deserializetoobject_isNull3 = true;\n/* 084 */       ArrayData deserializetoobject_value3 = null;\n/* 085 */       if (!inputadapter_isNull) {\n/* 086 */         deserializetoobject_isNull3 = false;\n/* 087 */         if (!deserializetoobject_isNull3) {\n/* 088 */           Object deserializetoobject_funcResult1 = null;\n/* 089 */           deserializetoobject_funcResult1 = inputadapter_value.valueArray();\n/* 090 */           if (deserializetoobject_funcResult1 == null) {\n/* 091 */             deserializetoobject_isNull3 = true;\n/* 092 */           } else {\n/* 093 */             deserializetoobject_value3 = (ArrayData) deserializetoobject_funcResult1;\n/* 094 */           }\n/* 095 */\n/* 096 */         }\n/* 097 */         deserializetoobject_isNull3 = deserializetoobject_value3 == null;\n/* 098 */       }\n/* 099 */       java.util.HashMap deserializetoobject_value = null;\n/* 100 */\n/* 101 */       if ((deserializetoobject_isNull1 && !deserializetoobject_isNull3) ||\n/* 102 */         (!deserializetoobject_isNull1 && deserializetoobject_isNull3)) {\n/* 103 */         throw new RuntimeException(\"Invalid state: Inconsistent nullability of key-value\");\n/* 104 */       }\n/* 105 */\n/* 106 */       if (!deserializetoobject_isNull1) {\n/* 107 */         if (deserializetoobject_value1.numElements() != deserializetoobject_value3.numElements()) {\n/* 108 */           throw new RuntimeException(\"Invalid state: Inconsistent lengths of key-value arrays\");\n/* 109 */         }\n/* 110 */         int deserializetoobject_dataLength = deserializetoobject_value1.numElements();\n/* 111 */         java.util.Map CollectObjectsToMap_builderValue5 = new java.util.HashMap(deserializetoobject_dataLength);\n/* 112 */\n/* 113 */         int deserializetoobject_loopIndex = 0;\n/* 114 */         while (deserializetoobject_loopIndex < deserializetoobject_dataLength) {\n/* 115 */           CollectObjectsToMap_loopValue0 = (int) (deserializetoobject_value1.getInt(deserializetoobject_loopIndex));\n/* 116 */           CollectObjectsToMap_loopValue2 = (int) (deserializetoobject_value3.getInt(deserializetoobject_loopIndex));\n/* 117 */           CollectObjectsToMap_loopIsNull1 = deserializetoobject_value1.isNullAt(deserializetoobject_loopIndex);\n/* 118 */           CollectObjectsToMap_loopIsNull3 = deserializetoobject_value3.isNullAt(deserializetoobject_loopIndex);\n/* 119 */\n/* 120 */           if (CollectObjectsToMap_loopIsNull1) {\n/* 121 */             throw new RuntimeException(\"Found null in map key!\");\n/* 122 */           }\n/* 123 */\n/* 124 */           CollectObjectsToMap_builderValue5.put(CollectObjectsToMap_loopValue0, CollectObjectsToMap_loopValue2);\n/* 125 */\n/* 126 */           deserializetoobject_loopIndex += 1;\n/* 127 */         }\n/* 128 */\n/* 129 */         deserializetoobject_value = (java.util.HashMap) CollectObjectsToMap_builderValue5;\n/* 130 */       }\n/* 131 */\n/* 132 */       boolean mapelements_isNull = true;\n/* 133 */       java.util.HashMap mapelements_value = null;\n/* 134 */       if (!false) {\n/* 135 */         mapelements_argValue = deserializetoobject_value;\n/* 136 */\n/* 137 */         mapelements_isNull = false;\n/* 138 */         if (!mapelements_isNull) {\n/* 139 */           Object mapelements_funcResult = null;\n/* 140 */           mapelements_funcResult = ((scala.Function1) references[0]).apply(mapelements_argValue);\n/* 141 */           if (mapelements_funcResult == null) {\n/* 142 */             mapelements_isNull = true;\n/* 143 */           } else {\n/* 144 */             mapelements_value = (java.util.HashMap) mapelements_funcResult;\n/* 145 */           }\n/* 146 */\n/* 147 */         }\n/* 148 */         mapelements_isNull = mapelements_value == null;\n/* 149 */       }\n/* 150 */\n/* 151 */       MapData serializefromobject_value = null;\n/* 152 */       if (!mapelements_isNull) {\n/* 153 */         final int serializefromobject_length = mapelements_value.size();\n/* 154 */         final Object[] serializefromobject_convertedKeys = new Object[serializefromobject_length];\n/* 155 */         final Object[] serializefromobject_convertedValues = new Object[serializefromobject_length];\n/* 156 */         int serializefromobject_index = 0;\n/* 157 */         final java.util.Iterator serializefromobject_entries = mapelements_value.entrySet().iterator();\n/* 158 */         while(serializefromobject_entries.hasNext()) {\n/* 159 */           final java.util.Map$Entry serializefromobject_entry = (java.util.Map$Entry) serializefromobject_entries.next();\n/* 160 */           int ExternalMapToCatalyst_key1 = (Integer) serializefromobject_entry.getKey();\n/* 161 */           int ExternalMapToCatalyst_value1 = (Integer) serializefromobject_entry.getValue();\n/* 162 */\n/* 163 */           boolean ExternalMapToCatalyst_value_isNull1 = false;\n/* 164 */\n/* 165 */           if (false) {\n/* 166 */             throw new RuntimeException(\"Cannot use null as map key!\");\n/* 167 */           } else {\n/* 168 */             serializefromobject_convertedKeys[serializefromobject_index] = (Integer) ExternalMapToCatalyst_key1;\n/* 169 */           }\n/* 170 */\n/* 171 */           if (false) {\n/* 172 */             serializefromobject_convertedValues[serializefromobject_index] = null;\n/* 173 */           } else {\n/* 174 */             serializefromobject_convertedValues[serializefromobject_index] = (Integer) ExternalMapToCatalyst_value1;\n/* 175 */           }\n/* 176 */\n/* 177 */           serializefromobject_index++;\n/* 178 */         }\n/* 179 */\n/* 180 */         serializefromobject_value = new org.apache.spark.sql.catalyst.util.ArrayBasedMapData(new org.apache.spark.sql.catalyst.util.GenericArrayData(serializefromobject_convertedKeys), new org.apache.spark.sql.catalyst.util.GenericArrayData(serializefromobject_convertedValues));\n/* 181 */       }\n/* 182 */       serializefromobject_holder.reset();\n/* 183 */\n/* 184 */       serializefromobject_rowWriter.zeroOutNullBytes();\n/* 185 */\n/* 186 */       if (mapelements_isNull) {\n/* 187 */         serializefromobject_rowWriter.setNullAt(0);\n/* 188 */       } else {\n/* 189 */         // Remember the current cursor so that we can calculate how many bytes are\n/* 190 */         // written later.\n/* 191 */         final int serializefromobject_tmpCursor = serializefromobject_holder.cursor;\n/* 192 */\n/* 193 */         if (serializefromobject_value instanceof UnsafeMapData) {\n/* 194 */           final int serializefromobject_sizeInBytes = ((UnsafeMapData) serializefromobject_value).getSizeInBytes();\n/* 195 */           // grow the global buffer before writing data.\n/* 196 */           serializefromobject_holder.grow(serializefromobject_sizeInBytes);\n/* 197 */           ((UnsafeMapData) serializefromobject_value).writeToMemory(serializefromobject_holder.buffer, serializefromobject_holder.cursor);\n/* 198 */           serializefromobject_holder.cursor += serializefromobject_sizeInBytes;\n/* 199 */\n/* 200 */         } else {\n/* 201 */           final ArrayData serializefromobject_keys = serializefromobject_value.keyArray();\n/* 202 */           final ArrayData serializefromobject_values = serializefromobject_value.valueArray();\n/* 203 */\n/* 204 */           // preserve 8 bytes to write the key array numBytes later.\n/* 205 */           serializefromobject_holder.grow(8);\n/* 206 */           serializefromobject_holder.cursor += 8;\n/* 207 */\n/* 208 */           // Remember the current cursor so that we can write numBytes of key array later.\n/* 209 */           final int serializefromobject_tmpCursor1 = serializefromobject_holder.cursor;\n/* 210 */\n/* 211 */           if (serializefromobject_keys instanceof UnsafeArrayData) {\n/* 212 */             final int serializefromobject_sizeInBytes1 = ((UnsafeArrayData) serializefromobject_keys).getSizeInBytes();\n/* 213 */             // grow the global buffer before writing data.\n/* 214 */             serializefromobject_holder.grow(serializefromobject_sizeInBytes1);\n/* 215 */             ((UnsafeArrayData) serializefromobject_keys).writeToMemory(serializefromobject_holder.buffer, serializefromobject_holder.cursor);\n/* 216 */             serializefromobject_holder.cursor += serializefromobject_sizeInBytes1;\n/* 217 */\n/* 218 */           } else {\n/* 219 */             final int serializefromobject_numElements = serializefromobject_keys.numElements();\n/* 220 */             serializefromobject_arrayWriter.initialize(serializefromobject_holder, serializefromobject_numElements, 4);\n/* 221 */\n/* 222 */             for (int serializefromobject_index1 = 0; serializefromobject_index1 < serializefromobject_numElements; serializefromobject_index1++) {\n/* 223 */               if (serializefromobject_keys.isNullAt(serializefromobject_index1)) {\n/* 224 */                 serializefromobject_arrayWriter.setNullInt(serializefromobject_index1);\n/* 225 */               } else {\n/* 226 */                 final int serializefromobject_element = serializefromobject_keys.getInt(serializefromobject_index1);\n/* 227 */                 serializefromobject_arrayWriter.write(serializefromobject_index1, serializefromobject_element);\n/* 228 */               }\n/* 229 */             }\n/* 230 */           }\n/* 231 */\n/* 232 */           // Write the numBytes of key array into the first 8 bytes.\n/* 233 */           Platform.putLong(serializefromobject_holder.buffer, serializefromobject_tmpCursor1 - 8, serializefromobject_holder.cursor - serializefromobject_tmpCursor1);\n/* 234 */\n/* 235 */           if (serializefromobject_values instanceof UnsafeArrayData) {\n/* 236 */             final int serializefromobject_sizeInBytes2 = ((UnsafeArrayData) serializefromobject_values).getSizeInBytes();\n/* 237 */             // grow the global buffer before writing data.\n/* 238 */             serializefromobject_holder.grow(serializefromobject_sizeInBytes2);\n/* 239 */             ((UnsafeArrayData) serializefromobject_values).writeToMemory(serializefromobject_holder.buffer, serializefromobject_holder.cursor);\n/* 240 */             serializefromobject_holder.cursor += serializefromobject_sizeInBytes2;\n/* 241 */\n/* 242 */           } else {\n/* 243 */             final int serializefromobject_numElements1 = serializefromobject_values.numElements();\n/* 244 */             serializefromobject_arrayWriter1.initialize(serializefromobject_holder, serializefromobject_numElements1, 4);\n/* 245 */\n/* 246 */             for (int serializefromobject_index2 = 0; serializefromobject_index2 < serializefromobject_numElements1; serializefromobject_index2++) {\n/* 247 */               if (serializefromobject_values.isNullAt(serializefromobject_index2)) {\n/* 248 */                 serializefromobject_arrayWriter1.setNullInt(serializefromobject_index2);\n/* 249 */               } else {\n/* 250 */                 final int serializefromobject_element1 = serializefromobject_values.getInt(serializefromobject_index2);\n/* 251 */                 serializefromobject_arrayWriter1.write(serializefromobject_index2, serializefromobject_element1);\n/* 252 */               }\n/* 253 */             }\n/* 254 */           }\n/* 255 */\n/* 256 */         }\n/* 257 */\n/* 258 */         serializefromobject_rowWriter.setOffsetAndSize(0, serializefromobject_tmpCursor, serializefromobject_holder.cursor - serializefromobject_tmpCursor);\n/* 259 */       }\n/* 260 */       serializefromobject_result.setTotalSize(serializefromobject_holder.totalSize());\n/* 261 */       append(serializefromobject_result);\n/* 262 */       if (shouldStop()) return;\n/* 263 */     }\n/* 264 */   }\n/* 265 */ }\n```\n\n## How was this patch tested?\n\n```\nbuild/mvn -DskipTests clean package && dev/run-tests\n```\n\nAdditionally in Spark shell:\n\n```\nscala> Seq(collection.mutable.HashMap(1 -> 2, 2 -> 3)).toDS().map(_ += (3 -> 4)).collect()\nres0: Array[scala.collection.mutable.HashMap[Int,Int]] = Array(Map(2 -> 3, 1 -> 2, 3 -> 4))\n```\n\nAuthor: Michal Senkyr <mike.senkyr@gmail.com>\nAuthor: Michal Šenkýř <mike.senkyr@gmail.com>\n\nCloses #16986 from michalsenkyr/dataset-map-builder."
}
{
  "message": "[SPARK-21031][SQL] Add `alterTableStats` to store spark's stats and let `alterTable` keep existing stats\n\n## What changes were proposed in this pull request?\n\nCurrently, hive's stats are read into `CatalogStatistics`, while spark's stats are also persisted through `CatalogStatistics`. As a result, hive's stats can be unexpectedly propagated into spark' stats.\n\nFor example, for a catalog table, we read stats from hive, e.g. \"totalSize\" and put it into `CatalogStatistics`. Then, by using \"ALTER TABLE\" command, we will store the stats in `CatalogStatistics` into metastore as spark's stats (because we don't know whether it's from spark or not). But spark's stats should be only generated by \"ANALYZE\" command. This is unexpected from this command.\n\nSecondly, now that we have spark's stats in metastore, after inserting new data, although hive updated \"totalSize\" in metastore, we still cannot get the right `sizeInBytes` in `CatalogStatistics`, because we respect spark's stats (should not exist) over hive's stats.\n\nA running example is shown in [JIRA](https://issues.apache.org/jira/browse/SPARK-21031).\n\nTo fix this, we add a new method `alterTableStats` to store spark's stats, and let `alterTable` keep existing stats.\n\n## How was this patch tested?\n\nAdded new tests.\n\nAuthor: Zhenhua Wang <wzh_zju@163.com>\n\nCloses #18248 from wzhfy/separateHiveStats."
}
{
  "message": "Fixed typo in sql.functions\n\n## What changes were proposed in this pull request?\n\nI fixed a typo in the Scaladoc for the method `def struct(cols: Column*): Column`. 'retained' was misspelt as 'remained'.\n\n## How was this patch tested?\nBefore:\n\nCreates a new struct column.\n   If the input column is a column in a `DataFrame`, or a derived column expression\n   that is named (i.e. aliased), its name would be **remained** as the StructField's name,\n   otherwise, the newly generated StructField's name would be auto generated as\n   `col` with a suffix `index + 1`, i.e. col1, col2, col3, ...\n\nAfter:\n\n   Creates a new struct column.\n   If the input column is a column in a `DataFrame`, or a derived column expression\n   that is named (i.e. aliased), its name would be **retained** as the StructField's name,\n   otherwise, the newly generated StructField's name would be auto generated as\n   `col` with a suffix `index + 1`, i.e. col1, col2, col3, ...\n\nAuthor: sujithjay <sujith@logistimo.com>\n\nCloses #18254 from sujithjay/fix-typo."
}
{
  "message": "[SPARK-20877][SPARKR][FOLLOWUP] clean up after test move\n\n## What changes were proposed in this pull request?\n\nclean up after big test move\n\n## How was this patch tested?\n\nunit tests, jenkins\n\nAuthor: Felix Cheung <felixcheung_m@hotmail.com>\n\nCloses #18267 from felixcheung/rtestset2."
}
{
  "message": "[SPARK-13933][BUILD] Update hadoop-2.7 profile's curator version to 2.7.1\n\n## What changes were proposed in this pull request?\n\nUpdate hadoop-2.7 profile's curator version to 2.7.1, more see [SPARK-13933](https://issues.apache.org/jira/browse/SPARK-13933).\n\n## How was this patch tested?\n\nmanual tests\n\nAuthor: Yuming Wang <wgyumg@gmail.com>\n\nCloses #18247 from wangyum/SPARK-13933."
}
{
  "message": "[SPARK-20935][STREAMING] Always close WriteAheadLog and make it idempotent\n\n## What changes were proposed in this pull request?\n\nThis PR proposes to stop `ReceiverTracker` to close `WriteAheadLog` whenever it is and make `WriteAheadLog` and its implementations idempotent.\n\n## How was this patch tested?\n\nAdded a test in `WriteAheadLogSuite`. Note that  the added test looks passing even if it closes twice (namely even without the changes in `FileBasedWriteAheadLog` and `BatchedWriteAheadLog`. It looks both are already idempotent but this is a rather sanity check.\n\nAuthor: hyukjinkwon <gurwls223@gmail.com>\n\nCloses #18224 from HyukjinKwon/streaming-closing."
}
{
  "message": "[SPARK-21000][MESOS] Add Mesos labels support to the Spark Dispatcher\n\n## What changes were proposed in this pull request?\n\nAdd Mesos labels support to the Spark Dispatcher\n\n## How was this patch tested?\n\nunit tests\n\nAuthor: Michael Gummelt <mgummelt@mesosphere.io>\n\nCloses #18220 from mgummelt/SPARK-21000-dispatcher-labels."
}
{
  "message": "[SPARK-20877][SPARKR] refactor tests to basic tests only for CRAN\n\n## What changes were proposed in this pull request?\n\nMove all existing tests to non-installed directory so that it will never run by installing SparkR package\n\nFor a follow-up PR:\n- remove all skip_on_cran() calls in tests\n- clean up test timer\n- improve or change basic tests that do run on CRAN (if anyone has suggestion)\n\nIt looks like `R CMD build pkg` will still put pkg\\tests (ie. the full tests) into the source package but `R CMD INSTALL` on such source package does not install these tests (and so `R CMD check` does not run them)\n\n## How was this patch tested?\n\n- [x] unit tests, Jenkins\n- [x] AppVeyor\n- [x] make a source package, install it, `R CMD check` it - verify the full tests are not installed or run\n\nAuthor: Felix Cheung <felixcheung_m@hotmail.com>\n\nCloses #18264 from felixcheung/rtestset."
}
{
  "message": "[SPARK-20620][TEST] Improve some unit tests for NullExpressionsSuite and TypeCoercionSuite\n\n## What changes were proposed in this pull request?\nadd more  datatype for some unit tests\n\n## How was this patch tested?\nunit tests\n\nAuthor: liuxian <liu.xian3@zte.com.cn>\n\nCloses #17880 from 10110346/wip_lx_0506."
}
{
  "message": "[SPARK-20211][SQL] Fix the Precision and Scale of Decimal Values when the Input is BigDecimal between -1.0 and 1.0\n\n### What changes were proposed in this pull request?\nThe precision and scale of decimal values are wrong when the input is BigDecimal between -1.0 and 1.0.\n\nThe BigDecimal's precision is the digit count starts from the leftmost nonzero digit based on the [JAVA's BigDecimal definition](https://docs.oracle.com/javase/7/docs/api/java/math/BigDecimal.html). However, our Decimal decision follows the database decimal standard, which is the total number of digits, including both to the left and the right of the decimal point. Thus, this PR is to fix the issue by doing the conversion.\n\nBefore this PR, the following queries failed:\n```SQL\nselect 1 > 0.0001\nselect floor(0.0001)\nselect ceil(0.0001)\n```\n\n### How was this patch tested?\nAdded test cases.\n\nAuthor: Xiao Li <gatorsmile@gmail.com>\n\nCloses #18244 from gatorsmile/bigdecimal."
}
{
  "message": "[SPARK-21042][SQL] Document Dataset.union is resolution by position\n\n## What changes were proposed in this pull request?\nDocument Dataset.union is resolution by position, not by name, since this has been a confusing point for a lot of users.\n\n## How was this patch tested?\nN/A - doc only change.\n\nAuthor: Reynold Xin <rxin@databricks.com>\n\nCloses #18256 from rxin/SPARK-21042."
}
{
  "message": "[SPARK-20918][SQL] Use FunctionIdentifier as function identifiers in FunctionRegistry\n\n### What changes were proposed in this pull request?\nCurrently, the unquoted string of a function identifier is being used as the function identifier in the function registry. This could cause the incorrect the behavior when users use `.` in the function names. This PR is to take the `FunctionIdentifier` as the identifier in the function registry.\n\n- Add one new function `createOrReplaceTempFunction` to `FunctionRegistry`\n```Scala\nfinal def createOrReplaceTempFunction(name: String, builder: FunctionBuilder): Unit\n```\n\n### How was this patch tested?\nAdd extra test cases to verify the inclusive bug fixes.\n\nAuthor: Xiao Li <gatorsmile@gmail.com>\nAuthor: gatorsmile <gatorsmile@gmail.com>\n\nCloses #18142 from gatorsmile/fuctionRegistry."
}
{
  "message": "[SPARK-20997][CORE] driver-cores' standalone or Mesos or YARN in Cluster deploy mode only.\n\n## What changes were proposed in this pull request?\n\n'--driver-cores'  standalone or Mesos or YARN in Cluster deploy mode only.So  The description of spark-submit about it is not very accurate.\n\n## How was this patch tested?\n\nmanual tests\n\nPlease review http://spark.apache.org/contributing.html before opening a pull request.\n\nAuthor: guoxiaolong <guo.xiaolong1@zte.com.cn>\nAuthor: 郭小龙 10207633 <guo.xiaolong1@zte.com.cn>\nAuthor: guoxiaolongzte <guo.xiaolong1@zte.com.cn>\n\nCloses #18241 from guoxiaolongzte/SPARK-20997."
}
{
  "message": "Fix bug in JavaRegressionMetricsExample.\n\nthe original code cant visit the last element of the\"parts\" array.\nso the v[v.length–1] always equals 0\n\n## What changes were proposed in this pull request?\nchange the recycle range from (1 to parts.length-1) to (1 to parts.length)\n\n## How was this patch tested?\n\ndebug it in eclipse (´〜｀*) zzz.\n\nPlease review http://spark.apache.org/contributing.html before opening a pull request.\n\nAuthor: junzhi lu <452756565@qq.com>\n\nCloses #18237 from masterwugui/patch-1."
}
{
  "message": "Fixed broken link\n\n## What changes were proposed in this pull request?\n\nI fixed some incorrect formatting on a link in the docs\n\n## How was this patch tested?\n\nI looked at the markdown preview before and after, and the link was fixed\n\nBefore:\n<img width=\"593\" alt=\"screen shot 2017-06-08 at 6 37 32 pm\" src=\"https://user-images.githubusercontent.com/17733030/26956272-a62cd558-4c79-11e7-862f-9d0e0184b18a.png\">\nAfter:\n<img width=\"587\" alt=\"screen shot 2017-06-08 at 6 37 44 pm\" src=\"https://user-images.githubusercontent.com/17733030/26956276-b1135ef6-4c79-11e7-8028-84d19c392fda.png\">\n\nAuthor: Corey Woodfield <coreywoodfield@gmail.com>\n\nCloses #18246 from coreywoodfield/master."
}
{
  "message": "[SPARK-20995][CORE] Spark-env.sh.template' should add 'YARN_CONF_DIR' configuration instructions.\n\n## What changes were proposed in this pull request?\n\nEnsure that `HADOOP_CONF_DIR` or `YARN_CONF_DIR` points to the directory which contains the (client side) configuration files for the Hadoop cluster.\nThese configs are used to write to HDFS and connect to the YARN ResourceManager. The\nconfiguration contained in this directory will be distributed to the YARN cluster so that all\ncontainers used by the application use the same configuration.\n\nSometimes, `HADOOP_CONF_DIR` is set to the hdfs configuration file path. So, YARN_CONF_DIR should be set to the yarn configuration file path.\n\nMy project configuration item of 'spark-env.sh ' is as follows:\n![1](https://cloud.githubusercontent.com/assets/26266482/26819987/d4acb814-4ad3-11e7-8458-a21aea57a53d.png)\n\n'HADOOP_CONF_DIR' configuration file path. List the relevant documents below:\n![3](https://cloud.githubusercontent.com/assets/26266482/26820116/47b6b9fe-4ad4-11e7-8131-fe07c8d8bc21.png)\n\n'YARN_CONF_DIR' configuration file path. List the relevant documents below:\n![2](https://cloud.githubusercontent.com/assets/26266482/26820078/274ad79a-4ad4-11e7-83d4-ff359dbb397c.png)\n\nSo, 'Spark-env.sh.template' should add 'YARN_CONF_DIR' configuration instructions.\n\n## How was this patch tested?\n\nmanual tests\n\nPlease review http://spark.apache.org/contributing.html before opening a pull request.\n\nAuthor: guoxiaolong <guo.xiaolong1@zte.com.cn>\nAuthor: 郭小龙 10207633 <guo.xiaolong1@zte.com.cn>\nAuthor: guoxiaolongzte <guo.xiaolong1@zte.com.cn>\n\nCloses #18212 from guoxiaolongzte/SPARK-20995."
}
{
  "message": "[SPARK-14408][CORE] Changed RDD.treeAggregate to use fold instead of reduce\n\n## What changes were proposed in this pull request?\n\nPreviously, `RDD.treeAggregate` used `reduceByKey` and `reduce` in its implementation, neither of which technically allows the `seq`/`combOps` to modify and return their first arguments.\n\nThis PR uses `foldByKey` and `fold` instead and notes that `aggregate` and `treeAggregate` are semantically identical in the Scala doc.\n\nNote that this had some test failures by unknown reasons. This was actually fixed in https://github.com/apache/spark/commit/e3554605b36bdce63ac180cc66dbdee5c1528ec7.\n\nThe root cause was, the `zeroValue` now becomes `AFTAggregator` and it compares `totalCnt` (where the value is actually 0). It starts merging one by one and it keeps returning `this` where `totalCnt` is 0. So, this looks not the bug in the current change.\n\nThis is now fixed in the commit. So, this should pass the tests.\n\n## How was this patch tested?\n\nTest case added in `RDDSuite`.\n\nCloses #12217\n\nAuthor: Joseph K. Bradley <joseph@databricks.com>\nAuthor: hyukjinkwon <gurwls223@gmail.com>\n\nCloses #18198 from HyukjinKwon/SPARK-14408."
}
{
  "message": "[SPARK-20863] Add metrics/instrumentation to LiveListenerBus\n\n## What changes were proposed in this pull request?\n\nThis patch adds Coda Hale metrics for instrumenting the `LiveListenerBus` in order to track the number of events received, dropped, and processed. In addition, it adds per-SparkListener-subclass timers to track message processing time. This is useful for identifying when slow third-party SparkListeners cause performance bottlenecks.\n\nSee the new `LiveListenerBusMetrics` for a complete description of the new metrics.\n\n## How was this patch tested?\n\nNew tests in SparkListenerSuite, including a test to ensure proper counting of dropped listener events.\n\nAuthor: Josh Rosen <joshrosen@databricks.com>\n\nCloses #18083 from JoshRosen/listener-bus-metrics."
}
{
  "message": "[SPARK-20954][SQL] `DESCRIBE [EXTENDED]` result should be compatible with previous Spark\n\n## What changes were proposed in this pull request?\n\nAfter [SPARK-20067](https://issues.apache.org/jira/browse/SPARK-20067), `DESCRIBE` and `DESCRIBE EXTENDED` shows the following result. This is incompatible with Spark 2.1.1. This PR removes the column header line in case of those command.\n\n**MASTER** and **BRANCH-2.2**\n```scala\nscala> sql(\"desc t\").show(false)\n+----------+---------+-------+\n|col_name  |data_type|comment|\n+----------+---------+-------+\n|# col_name|data_type|comment|\n|a         |int      |null   |\n+----------+---------+-------+\n```\n\n**SPARK 2.1.1** and **this PR**\n```scala\nscala> sql(\"desc t\").show(false)\n+--------+---------+-------+\n|col_name|data_type|comment|\n+--------+---------+-------+\n|a       |int      |null   |\n+--------+---------+-------+\n```\n\n## How was this patch tested?\n\nPass the Jenkins with the updated test suites.\n\nAuthor: Dongjoon Hyun <dongjoon@apache.org>\n\nCloses #18203 from dongjoon-hyun/SPARK-20954."
}
{
  "message": "[SPARK-20976][SQL] Unify Error Messages for FAILFAST mode\n\n### What changes were proposed in this pull request?\nBefore 2.2, we indicate the job was terminated because of `FAILFAST` mode.\n```\nMalformed line in FAILFAST mode: {\"a\":{, b:3}\n```\nIf possible, we should keep it. This PR is to unify the error messages.\n\n### How was this patch tested?\nModified the existing messages.\n\nAuthor: Xiao Li <gatorsmile@gmail.com>\n\nCloses #18196 from gatorsmile/messFailFast."
}
{
  "message": "[SPARK-19185][DSTREAM] Make Kafka consumer cache configurable\n\n## What changes were proposed in this pull request?\n\nAdd a new property `spark.streaming.kafka.consumer.cache.enabled` that allows users to enable or disable the cache for Kafka consumers. This property can be especially handy in cases where issues like SPARK-19185 get hit, for which there isn't a solution committed yet. By default, the cache is still on, so this change doesn't change any out-of-box behavior.\n\n## How was this patch tested?\nRunning unit tests\n\nAuthor: Mark Grover <mark@apache.org>\nAuthor: Mark Grover <grover.markgrover@gmail.com>\n\nCloses #18234 from markgrover/spark-19185."
}
{
  "message": "[INFRA] Close stale PRs\n\n# What changes were proposed in this pull request?\n\nThis PR proposes to close stale PRs, mostly the same instances with https://github.com/apache/spark/pull/18017\n\nCloses #11459\nCloses #13833\nCloses #13720\nCloses #12506\nCloses #12456\nCloses #12252\nCloses #17689\nCloses #17791\nCloses #18163\nCloses #17640\nCloses #17926\nCloses #18163\nCloses #12506\nCloses #18044\nCloses #14036\nCloses #15831\nCloses #14461\nCloses #17638\nCloses #18222\n\nAdded:\nCloses #18045\nCloses #18061\nCloses #18010\nCloses #18041\nCloses #18124\nCloses #18130\nCloses #12217\n\nAdded:\nCloses #16291\nCloses #17480\nCloses #14995\n\nAdded:\nCloses #12835\nCloses #17141\n\n## How was this patch tested?\n\nN/A\n\nAuthor: hyukjinkwon <gurwls223@gmail.com>\n\nCloses #18223 from HyukjinKwon/close-stale-prs."
}
